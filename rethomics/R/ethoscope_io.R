#@include
NULL
#' Read data from a result file.
#' 
#' This function is used to convert all the information
#' contained in a result file generated by the ethoscope platform \url{http://gilestrolab.github.io/ethoscope/}
#' (i.e a .db file) into an R `data.table'.
#'
#' @param what an object describing which file(s) to load and, optionally, associated variables/conditions (see details).
#' @param min_time exclude data before min_time (in seconds). This time is relative to the start of the experiment.
#' @param max_time exclude data after max_time (in seconds). It is also relative to the start of the experiment.
#' @param reference_hour the hour, in the day, to use as t_0 reference. When unspecified, time will be relative to the start of the experiment.
#' @param verbose whether to print progress (a logical).
#' @param cache_files whether SQL files should be cached in a tmp dir for faster reading
#' @param n_cores how many cores should be used to read/convert data
#' @param columns an optionnal vector of columns to be selected from the db file. Time (t) is always implicitely selected.
#' @param FUN an optional function to transform the data from each `region' (i.e. a data.table) immediately after is has been loaded. 
#' @param ... extra arguments to be passed to \code{FUN}
#' @return A data.table where every row is an individual measurement. That is a position at a unique time (\code{t}) in a 
#' unique region (\code{region_id}), and from a unique result file/experiment (\code{experiment_id}).
#' The time is expressed in seconds. Distance units (e.g. xy position, height/width) are expressed as a fraction of the width of the region they originate from.
#' @details \code{what} can be one of two objects:
#' \itemize{
#'  \item{A character vector. }{In which case, it is assumed that each element is the path to a different file to load.}
#'  \item{A dataframe. }{The dataframe \emph{must} have a column named `path'. 
#'  The path basename will be used as a unique identifier for a specific experiment (\code{experiment_id}).
#' Arbitrary column can be added to map experimental conditions to file name.
#' In addition, the dataframe can have a column named \code{region_id}. When defined, only the specified combinations of \code{path} and \code{region_id}
#' will be loaded. This allows to map additional conditions (i.e. data frame columns) to specific regions/files.
#' When additional conditions are provided, they will result in creation of custom columns in the output of this function.}
#' }
#'
#' @examples
#'
#' # First of all, let us load files from the data sample included within this package.
#' # Most likely, you will already have your own data files.
#' sample_files <- c("tube_monitor_validation_subset.db",
#'                   "monitor_validation_subset.db")
#' # Extract the files in your computer
#' paths <- sapply(sample_files, loadSampleData)
#' # Now, `paths` is just a vector of file names:
#' print(paths)
#' #################
#' #################
#' # Case 1: load ALL REGIONS from a SINGLE FILE
#' validation_data_file <- paths[1]
#' # `validation_data_file` is simply the path to the .db file in your computer
#' dt <- loadEthoscopeData(validation_data_file)
#' print(dt)
#' ###############
#' # Case 2: load ALL REGIONS from MULTIPLE FILES
#' # we pass all the files we want to load as the `what` argument
#' dt <- loadEthoscopeData(paths)
#' # Note the column `experiment_id` in dt. It tells us which file/experiment 
#' # each measurement originates from.
#' print(dt)
#'
#' ###############
#' # Case 3: load ALL REGIONS from MULTIPLE FILES AND add CONDITIONS
#' # Let us imagine that each file/experiment
#' # was acquired under different experimental condition.
#' # We can encode this information in a 'master-table' (i.e a data.frame) 
#' # in which a column named \code{path} maps experimental condition(s). 
#' # For instance, 2 different treatments:
#' master_table <- data.frame(path=paths, treatment=c("control", "drug_A"))
#' # Let us check our table:
#' print(master_table)
#' # The table looks OK, so we load the actual data
#' dt <- loadEthoscopeData(master_table)
#' # Note that `dt` now contains a column for your treatment.
#' print(colnames(dt))
#' # This makes it easier to perform things such as average per treatment.
#' print(dt[,.(mean_x = mean(x)),by="treatment"])
#' ###############
#' # Case 4: load SELECTED REGIONS from MULTIPLE FILE, WITH CONDITIONS
#' # Sometimes, different regions contain different conditions.
#' # If the master table has a column named `region_id`, 
#' # only the specified regions will be returned.
#' # Let us assume that we want to replicate case 3, 
#' # but, now, we load only the first 20 regions.
#' master_table <- data.table(path=paths, 
#'                            treatment=c("control", "drug_A"), 
#'                            region_id=rep(1:20,each= 2))
#' # We could also imagine that every even region contains a male,
#' # whilst every odd one has a female:
#' master_table[, sex := ifelse(region_id %% 2, "male", "female" )]
#' # Note that we have now two conditions.
#' # Let us check our new table:
#' print(master_table)
#' # Then we can load our data:
#' dt <- loadEthoscopeData(master_table)
#' # This is simply a subset of data, so many regions are missing
#' # lets display the regions we ended up with
#' print(dt[,.(NA),by=key(dt)])
#' ####################
#' # Case 5: Apply ANALYSIS/function whist loading the data.
#' # You can also apply a function from this package,
#' # or your own function to the data as it is being loaded.
#' # For instance, if you wish to peform a `sleep annotation':
#' dt <- loadEthoscopeData(paths[1], FUN=sleepAnnotation)
#' # You could of course combine this with more conditions/region selection.
#' # For most complicated cases, you would probably have pre-generated the 
#' # master-table (e.g. as a csv file) before analysing the results.

#' @seealso \code{\link{loadEthoscopeMetaData}} To display global informations about a specific file. 
#' @export
loadEthoscopeData <- function(what,
                              min_time = 0,
                              max_time = Inf, 
                              reference_hour=NULL,
                              verbose=TRUE,
                              cache_files=TRUE,
                              n_cores=1,
                              columns = NULL,
                              FUN=NULL,
                              ...){
  
  # from the `what` argument, we build a `master_table` that we will map to the actual data.
  
  # case 1 what is a file, or a vector of files
  if(is.character(what)){
    # todo check whether file exists
    master_table <- data.table(path=what, experiment_id=basename(what))
    # We load all available ROIs since user did not provide ROI info
    master_table <- master_table[,list(
      region_id=availableROIs(path),
      experiment_id=experiment_id),by=path]
  }
  else if(is.data.frame(what)){
    
    checkColumns("path", colnames(what))
    
    master_table <- copy(as.data.table(what))
    #fixme check uniqueness of file/use path as key?
    master_table[,path := as.character(path)]
    master_table[,experiment_id := basename(path)]
    
    setkey(master_table,experiment_id)
    
    if(!"region_id" %in% colnames(what)){
      m <- master_table[,list(region_id=availableROIs(path)),by=key(master_table)]
      master_table <- m[master_table]
    }
    
  }
  else{
    stop("Unexpected `what` argument!")
  }
  
  if(cache_files)
    master_table[, path:= cacheResultFile(master_table[,path])]
  
  setkeyv(master_table,c("experiment_id","region_id"))
  
  if(n_cores == 1)
    l_dt <- lapply(1:nrow(master_table),parseOneROI, master_table,min_time, max_time, reference_hour,verbose,columns=columns,FUN,...)
  else{
    require(parallel)
    #cl <- makeCluster(n_cores)
    l_dt <- mclapply(1:nrow(master_table),parseOneROI, master_table,min_time, max_time, reference_hour,verbose,columns,FUN,...,mc.cores = getOption("mc.cores", n_cores))
    # stopCluster(cl)
  }
  
  l_dt <- l_dt[!sapply(l_dt,is.null)]
  if(length(unique(lapply(l_dt,key))) > 1){
    stop("Data tables do not have the same keys")
  }
  
  keys <- key(l_dt[[1]])
  out <- rbindlist(l_dt)
  rm(l_dt)
  
  setkeyv(out, keys)
  out <- master_table[out]
  out$path <-NULL
  master_table$path <-NULL
  master_table
  setkeyv(out, colnames(master_table))
  out
}

NULL
#'  Build a query for loading ethoscope dat; using the date of experiments and devices name to retreive result files.
#' 
#' This function is designed to list and select experimental files. 
#' In general, end-users will want to retrieve  path to their experimental files
#' according to the date and ID of the video monitor without having to understand the underlying directory structure.
#' @param result_dir The location of the result directory (i.e. the folder containing all the data).
#' @param query An optional query formatted as a dataframe (see details).
#' @return
#' The query extended with the requested paths. When \code{query} is not specified, the function returns a table with all available files.
#' @details
#' The optional argument \code{query} is expected to be a table where every row maps an experiment.
#' In many respects, it is similar to the \code{what} argument in \code{\link{loadEthoscopeData}}.
#' The only difference is that it does not have a \code{path} column. Instead, it must contain two columns:
#' \itemize{
#'  \item{\code{date} }{The date and time when the experiment started formatted either as `yyyy-mm-dd' or `yyyy-mm-dd_hh:mm:ss'.
#'  In the former case, there may be several matching experiments to a single time (starting the same day).
#'  When this happens, \emph{only the last} is returned, and a warning message is displayed.}
#'  \item{\code{machine_name} }{The name of the machine that acquired the data.}
#'}
#' The result is meant to be used directly, as the \code{what} argument, by \code{\link{loadEthoscopeData}} (see examples).
#' @note
#' The ethoscope platform store data in a hard-coded directory structure \code{/root_dir/machine_id/machine_name/datetime/file.db}, where:
#' \itemize{
#'  \item{\code{machine_id} }{Is, in principle, a universally unique identifier of the acquisition device.}
#'  \item{\code{machine_name}, }{a human friendly name for acquisition device. In practice, this is expected to be unique within laboratory.}
#'  \item{\code{datetime}, }{the date and time of the start of the experiment}
#' }
#' @examples
#' \dontrun{
#' # This is where I store the data on my computer
#' MY_DATA_DIR <- "/data/ethoscope_results/"
#' 
#' query <- data.table(date="2015-06-02",
#'                     machine_name=c("GGSM-001","GGSM-003"),
#'                     region_id = rep(1:10,each=2))
#' print(query)
#' map <- buildEthoscopeQuery(MY_DATA_DIR, query)
#' dt <- loadEthoscopeData(map)
#' }
#' @export
buildEthoscopeQuery <- function(result_dir,query=NULL){
  
  checkDirExists(result_dir)
  
  key <- c("date","machine_name")
  use_date <- F
  if(!is.null(query)){
    q <- copy(as.data.table(query))
    checkColumns(key, colnames(q))
    q[, date:=as.character(date)]
    
    t <- q[,as.POSIXct(date, "%Y-%m-%d_%H-%M-%S", tz="GMT")]
    if(any(is.na(t)))
      t <- q[,as.POSIXct(date, "%Y-%m-%d", tz="GMT")]
    use_date <- T
    
    q[,date :=t]
    setkeyv(q,key)
  }
  all_db_files <- list.files(result_dir,recursive=T, pattern="*\\.db$")
  
  
  fields <- strsplit(all_db_files,"/")
  valid_files <- sapply(fields,length) == 4
  
  all_db_files <- all_db_files[valid_files]
  
  if(length(all_db_files) == 0){
    stop(sprintf("No .db files detected in the directory '%s'. Ensure it is not empty.",result_dir))
  }
  
  files_info <- do.call("rbind",fields[valid_files])
  files_info <- as.data.table(files_info)
  setnames(files_info, c("machine_id", "machine_name", "date","file"))
  
  if(use_date)
    files_info[,date:=as.POSIXct(date, "%Y-%m-%d", tz="GMT")]
  else
    files_info[,date:=as.POSIXct(date, "%Y-%m-%d_%H-%M-%S", tz="GMT")]
  
  files_info[,path := paste(result_dir,all_db_files,sep="/")]
  setkeyv(files_info,key)
  
  if(is.null(query))
    return(files_info)
  files_info[,n:=.N,by=key(files_info)]
  unique_fi = unique(files_info,fromLast = T)
  out <- unique_fi[q]
  duplicated_queries <- unique(out[n>1,.(date,machine_name)])
  if(nrow(duplicated_queries) > 0){
    for( i in 1:nrow(duplicated_queries)){
      str <- "Duplicated queries. Excluding {%s, %s}"        
      str <- sprintf(str,duplicated_queries[i,machine_name],duplicated_queries[i,date])
      warning(str)
    }
  }
  # we don't need the column 'n' any longer
  out[, n:=NULL]
  nas <- is.na(out[,path]) 
  if(any(nas)){
    out_nas <- out[nas,]
    for(i in nrow(out_nas)){
      warning(sprintf("No result for machine_name == %s and date == %s. Omiting query",out_nas[i,machine_name],out_nas[i,date])) 
    }
  }
  out <- na.omit(out)
  setkeyv(out, union(key(out),colnames(q)))
  out
}

#' Retrieves  metadata from a result file.
#' 
#' This function is used to obtain metadata -- such as `time and date of the experiment' , `acquisition device', `version of the software' and such--
#' embedded in a result file generated by the ethoscope platform.
#'
#' @param FILE the name of the input file.
#' @return A list containing fields for metadata entries
#' @examples
#' \dontrun{
#' FILE <- "result.db"
#' out <- loadEthoscopeMetaData(FILE)
#' names(out)
#' }
#' @seealso \code{\link{loadEthoscopeData}} to load raw data. 
#' @export
loadEthoscopeMetaData <- function(FILE){
  con <- dbConnect(SQLite(), FILE)
  metadata <- dbGetQuery(con, "SELECT * FROM METADATA")
  dbDisconnect(con)
  v <- as.list(metadata$value)
  names(v) <- metadata$field
  #fixme explicitly GMT
  v$date_time <- as.POSIXct(as.integer(v$date_time),origin="1970-01-01",tz = "GMT")
  return(v)		
}

availableROIs <- function(FILE){
  con <- dbConnect(SQLite(), FILE)
  roi_map <- as.data.table(dbGetQuery(con, "SELECT * FROM ROI_MAP"))
  setkey(roi_map, roi_idx)
  
  available_rois  <- roi_map[ ,roi_idx]
  dbDisconnect(con)
  return(available_rois)
}

parseOneROI <- function(i, master_table,min_time, max_time, reference_hour,verbose,FUN,columns=NULL,...){
  
  region_id <- master_table[i,region_id]
  experiment_id <- master_table[i,experiment_id]
  path <- master_table[i,path]
  if(verbose)
    cat(sprintf("Loading ROI number %i from:\n\t%s\n",region_id,path))
  
  out <- loadOneROI(path,	region_id=region_id,
                    min_time = min_time,
                    max_time = max_time, 
                    reference_hour=reference_hour,columns=columns)
  
  if(is.null(out) || nrow(out) == 0){
    warning(sprintf("No data in ROI %i, from FILE %s. Skipping",region_id, path))
    return(NULL)
  }
  
  if(!is.null(FUN)){
    out <- FUN(out,...)
    if(is.null(out)){
      warning(sprintf("No data in ROI %i after running FUN, from FILE %s. Skipping",region_id, path))
      return(NULL)
    }
  }
  
  out[,experiment_id:=experiment_id]
  setkeyv(out,c("experiment_id","region_id"))
}

# a helper function to laod data from a single region
loadOneROI <- function( FILE,  region_id, min_time=0, max_time=Inf,  reference_hour=NULL, columns = NULL){
  metadata <- loadEthoscopeMetaData(FILE)
  con <- dbConnect(SQLite(), FILE)
  
  var_map <- as.data.table(dbGetQuery(con, "SELECT * FROM VAR_MAP"))
  setkey(var_map, var_name)
  roi_map <- as.data.table(dbGetQuery(con, "SELECT * FROM ROI_MAP"))
  roi_row <- roi_map[roi_idx == region_id,]
  if(nrow(roi_row) == 0 ){
    warning(sprintf("ROI %i does not exist, skipping",region_id))
    return(NULL)
  }
  if(max_time == Inf)
    max_time_condition <- ""
  else
    max_time_condition <-  sprintf("AND t < %e", max_time * 1000) 
  
  min_time <- min_time * 1000 
  
  if(is.null(columns)){
    selected_cols = "*"
  }
  else{
    
    if("is_inferred" %in% var_map$var_name)
      columns <- unique(c(columns, "is_inferred"))
    
    if(any(!columns %in% var_map$var_name))
      stop(sprintf("Some of the requested columns are NOT available. Available columns are: %s",paste(var_map$var_name, collapse = " ")))
    
    selected_cols = paste(unique(c("t",columns)),collapse=", ")
    var_map <- var_map[columns]
    setkey(var_map, var_name)
  }
  
  sql_query <- sprintf("SELECT %s FROM ROI_%i WHERE t >= %e %s",selected_cols, region_id,min_time, max_time_condition )
  
  roi_dt <- as.data.table(dbGetQuery(con, sql_query))
  if("id" %in% colnames(roi_dt))
    roi_dt$id <- NULL
  roi_dt[, region_id := region_id]
  
  if(!is.null(reference_hour)){
    p <- metadata$date_time
    hour_start <- as.numeric(format(p, "%H")) + as.numeric(format(p, "%M")) / 60 +  as.numeric(format(p, "%S")) / 3600
    ms_after_ref <- ((hour_start - reference_hour) %% 24) * 3600 * 1000
    roi_dt[, t:= (t + ms_after_ref) ]
  }
  
  #time_in_seconds
  roi_dt[, t:= t/1e3]
  
  roi_width <- max(c(roi_row[,w], roi_row[,h]))
  for(var_n in var_map$var_name){
    if(var_map[var_n, functional_type] == "distance"){
      roi_dt[, (var_n) := get(var_n) / roi_width]
    }
    if(var_map[var_n, sql_type] == "BOOLEAN"){
      roi_dt[, (var_n) := as.logical(get(var_n))]
    }
  }
  if("is_inferred" %in% colnames(roi_dt))
    roi_dt <- roi_dt[is_inferred==F]
  roi_dt$is_inferred <- NULL
  return(roi_dt)
}
NULL
cacheResultFile <- function(all_paths,subdir="rethomic_file_cache"){
  src <- unique(all_paths)
  
  dst_dir <- paste(tempdir(),subdir,sep="/")
  if(!dir.exists(dst_dir))
    dir.create(dst_dir)
  dst <- paste(dst_dir,basename(src),sep="/")
  map <- data.table(src,dst,key="src")
  
  lapply(1:nrow(map),function(i){
    file.copy(map[i,src],map[i,dst],overwrite = F)
  })
  path_map <- data.table(src=all_paths)
  return(map[path_map][,dst])
}
NULL