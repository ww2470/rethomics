#' Read data from an ethoscope result file
#' 
#' This function is used to convert all the information
#' contained in a result file generated by the ethoscope platform \url{http://gilestrolab.github.io/ethoscope/}
#' (i.e a .db file) into an `R` `data.table`.
#'
#' @param what Query describing which file(s) to load and, optionally, associated variables/conditions (see details).
#' @param min_time Exclude data before min_time (in seconds). 
#' This time is *relative to the start of the experiment*.
#' @param max_time Exclude data after max_time (in seconds). 
#' It is also relative to the start of the experiment.
#' @param reference_hour Hour, in the day, to use as ZT0 reference.
#' When unspecified, time will be relative to the start of the experiment.
#' @param verbose whether to print progress (a logical).
#' @param columns Optionnal vector of columns to be selected from the db file.
#' Time (t) is always implicitely selected.
#' @param ncores Number of cores to use for optionnal parallel processing.
#' @param FUN Optional function to transform the data from each `region_id` (i.e. a `data.table`)
#' immediately after is has been loaded. 
#' @param ... Extra arguments to be passed to `FUN`
#' @return A data.table where every row is an individual measurement. 
#' That is a position at a unique time (`t`) in a 
#' unique region (`region_id`), and from a unique result file/experiment (`experiment_id`).
#' The time is expressed in seconds. 
#' Distance units (e.g. xy position, height/width) are expressed as a fraction of the width of the region
#'  they originate from.
#' @details 
#' `what` can be either a:
#' 
#'  * **character vector** In which case, it is assumed that each element is the path to a different file to load.
#'  * **data.table** to use as a *query*. In which case, it **must** have a column named `"path"`. 
#'  
#'  In general, you will use [buildEthoscopeQuery] to help you build the query.
#'  The path basename will be used as a unique identifier for a specific experiment (`experiment_id`).
#' Generally, arbitrary column will be added to the query to map experimental conditions to file name.
#' In addition, the query can have a column named `region_id`. 
#' When defined, only the specified combinations of `path` and `region_id` will be loaded.
#' This allows to map additional conditions (i.e. `data.table` columns) to specific `region_id x experiment_id`.
#' When additional conditions are provided, 
#' they will result in creation of custom columns in the output of this function.}
#' 
#' @examples
#' \donttest{
# ################
#' # Case 1: load ALL REGIONS from a SINGLE FILE
#' # we can download a file from a repo:
#' #TODO
#' data_file <- "/somewhere/in/my/computer/file.db"
#' # `validation_data_file` is simply the path to the .db file in your computer
#' dt <- loadEthoscopeData(validation_data_file)
#' print(dt)
# ###############
#' # Case 2: load ALL REGIONS from MULTIPLE FILES
#' # we pass all the files we want to load as the `what` argument
#' dt <- loadEthoscopeData(paths)
#' # Note the column `experiment_id` in dt. It tells us which file/experiment 
#' # each measurement originates from.
#' print(dt)
#
# ###############
# # Case 3: load ALL REGIONS from MULTIPLE FILES AND add CONDITIONS
# # Let us imagine that each file/experiment
# # was acquired under different experimental condition.
# # We can encode this information in a 'query' (i.e a data.frame) 
# # in which a column named \code{path} maps experimental condition(s). 
# # For instance, 2 different treatments:
# query <- data.frame(path=paths, treatment=c("control", "drug_A"))
# # Let us check our table:
# print(query)
# # The table looks OK, so we load the actual data
# dt <- loadEthoscopeData(query)
# # Note that `dt` now contains a column for your treatment.
# print(colnames(dt))
# # This makes it easier to perform things such as average per treatment.
# print(dt[,.(mean_x = mean(x)),by="treatment"])
# ###############
# # Case 4: load SELECTED REGIONS from MULTIPLE FILE, WITH CONDITIONS
# # Sometimes, different regions contain different conditions.
# # If the query has a column named `region_id`, 
# # only the specified regions will be returned.
# # Let us assume that we want to replicate case 3, 
# # but, now, we load only the first 20 regions.
# query <- data.table(path=paths, 
#                            treatment=c("control", "drug_A"), 
#                            region_id=rep(1:20,each= 2))
# # We could also imagine that every even region contains a male,
# # whilst every odd one has a female:
# query[, sex := ifelse(region_id %% 2, "male", "female" )]
# # Note that we have now two conditions.
# # Let us check our new table:
# print(query)
# # Then we can load our data:
# dt <- loadEthoscopeData(query)
# # This is simply a subset of data, so many regions are missing
# # lets display the regions we ended up with
# print(dt[,.N,by=key(dt)])
# 
# # For most complicated cases, you would probably have pre-generated the 
# # query (e.g. as a csv file) before analysing the results.
#' }
#'
#' @seealso
#' * [buildEthoscopeQuery] to generate a query with the `path` of the data file
#' * Tutorial for this function \url{http://gilestrolab.github.io/rethomics/tutorial/todo}
#' * What queries are \url{http://gilestrolab.github.io/rethomics/tutorial/todo}
#' * The structure of the resuting data \url{http://gilestrolab.github.io/rethomics/tutorial/todo}
#' @export
loadEthoscopeData <- function(what,
                              min_time = 0,
                              max_time = Inf, 
                              reference_hour=NULL,
                              verbose=TRUE,
                              columns = NULL,
                              ncores=1,
                              FUN=NULL,
                              ...){
  # from the `what` argument, we build a `master_table` that we will map to the actual data.
  master_table <- makeMasterTable(what)
  
  # Each row of master table refers to a unique ROI. to each ROI we apply the function `parseOneROI` 
  # and get each ROI in a dt.
  # So, l_dt is a list of data tables, one per ROI. If no data is availeble, the list element is `NULL`.
  
  if(ncores == 1){
    l_dt <- lapply(1:nrow(master_table),parseOneROI, master_table,min_time, max_time, reference_hour,verbose,columns=columns,FUN,...)
  }
  else{
    if (!requireNamespace("parallel", quietly = TRUE)) {
      stop("`parallel` package needed for ncores > 1.
           Please install it.",
           call. = FALSE)
    }
    # cl <- makeCluster(getOption("cl.cores", ncores))
    # clusterExport(cl, "master_table")
    l_dt <- parallel::mclapply(1:nrow(master_table),parseOneROI, 
                      mc.cores=ncores,
                      master_table,min_time, max_time,
                      reference_hour, verbose,columns=columns,FUN,...)
  }

  # if any element of the list is `NULL`, then it is removed
  l_dt <- l_dt[!sapply(l_dt,is.null)]
  
  #all data table in the list should have the same key
  if(length(unique(lapply(l_dt,key))) > 1){
    stop("Data tables do not have the same keys")
  }
  # if they have the same keys, then  the key for everyone is:
  keys <- key(l_dt[[1]])
  
  # then we bin ALL roi tables in one datatable with all ROIs in  
  out <- rbindlist(l_dt)
  
  # now we remove l_dt as it is expected to be quite large
  rm(l_dt)
  # we also can force R to garbage collect, making memory avalable:
  gc()
  
  # we reasign the correct key to the output data table
  setkeyv(out, keys)
  
  # we join master and out, so extra, user defined, cols are no in out
  out <- master_table[out]
  
  # we don't want path in out, it is too long and inconsistent
  out[, path := NULL]
  
  master_table[, path := NULL]
  
  # we want all user defined variables AND the default key (region_id, experiment_id) to be key.
  setkeyv(out, colnames(master_table))
  out
}

# Generates a unified master table for any allowed `what` argument.
# The resulting master table will always have the columns "path", "experiment_id", "region_id"
makeMasterTable <- function(what){
  
  # case 1 what is a file, or a vector of files
  if(is.character(what)){
    # todo check whether file exists
    master_table <- data.table(path=what, experiment_id=basename(what))
    # We load all available ROIs since user did not provide ROI info
    master_table <- master_table[,list(
      region_id=availableROIs(path),
      experiment_id=experiment_id),by=path]
  }
  else if(is.data.frame(what)){
    checkColumns("path", what)
    
    master_table <- copy(as.data.table(what))
    #fixme check uniqueness of file/use path as key?
    master_table[,path := as.character(path)]
    master_table[,experiment_id := basename(path)]
    
    setkey(master_table,experiment_id)
    
    # when user did not specify ROIs, we load them all.
    if(!"region_id" %in% colnames(what)){
      m <- master_table[,list(region_id=availableROIs(path)),by=key(master_table)]
      master_table <- m[master_table]
    }
    
  }
  else{
    stop("Unexpected `what` argument!")
  }
  
  checkColumns(c("experiment_id","region_id","path"), master_table)
  
  setkeyv(master_table,c("experiment_id","region_id"))
  master_table[,n:=.N,by=key(master_table)]
  master_table <- unique(master_table, by=key(master_table))
  duplicated_rows <- master_table[n>1]
  
  if(nrow(duplicated_rows)>0){
    for(i in 1:nrow(duplicated_rows)){
      str <- "Duplicated rows in query! experiment %s and region %i}"        
      str <- sprintf(str, duplicated_rows[i,experiment_id],duplicated_rows[i,region_id])
      warning(str)
    }
      
  }
  master_table[,n:=NULL]
  return(master_table)
}

NULL
#' Build a functional query for loading ethoscope data using the date of experiments and device names
#' to retreive result files
#' 
#' This function is designed to list and select experimental files. 
#' In general, end-users will want to retrieve  path to their experimental files
#' according to the date and ID of the ethoscope without having to understand the underlying directory structure.
#' 
#' @param result_dir The location of the result directory (i.e. the folder containing all the data).
#' @param use_cached whether cache files should be used
#' @param query An optional query formatted as a dataframe (see details).
#' @param index_file An optional file listing all experiments.
#' @return Query extended with the requested paths. 
#' When `query` is not specified, the function returns a table with all available files.
#' @details
#' The optional argument `query` is expected to be a table where each row maps an experiment.
#' In many respects, it is similar to the `what` argument in [loadEthoscopeData].
#' The only difference is that it does not have a `"path"` column. 
#' Instead, it must contain two columns:
#' 
#'  * `date`  the date and time when the experiment started formatted either as `yyyy-mm-dd` or
#'  `yyyy-mm-dd_hh:mm:ss`. In the former case, there may be several matching experiments to a 
#'  single time (starting the same day). When this happens, **only the last** is returned,
#'  and a warning message is displayed.
#'  * `machine_name` The name of the machine that acquired the data.
#'
#' The result is meant to be used directly, as the `what` argument, in [loadEthoscopeData] (see examples).
#' @note The ethoscope platform store data in a hard-coded directory structure
#' `"/root_dir/machine_id/machine_name/datetime/file.db"`, where:
#' 
#'  * `machine_id` Is, in principle, a universally unique identifier of the acquisition device.
#'  * `machine_name` a human friendly name for acquisition device. In practice, this is expected to be unique within laboratory.
#'  * `datetime` the date and time of the start of the experiment
#'  
#'  @seealso todo
# @seealso \code{\link{cacheEthoscopeData}} to build a cached data directory.

#' @examples
#' \donttest{
#' # Some sample data are store in the package:
#' sample_dir <- getSampleDataPath(get_sample_dir =  TRUE)
#' result_dir <- paste(sample_dir,"ethoscope",sep="/")
#'
#' query <- data.table(
#'    machine_name = c("E_029","E_014", "E_014"),
#'    date = c("2016-01-25","2016-01-25", "2016-02-17")
#'    )
#' query_path <- buildEthoscopeQuery(result_dir, query)
#' # now we have maped a query to a path:
#' print(query)
#' }
#' @export
buildEthoscopeQuery <- function(result_dir, query=NULL, use_cached=FALSE, index_file=NULL){
  if(is.null(index_file))
	checkDirExists(result_dir)
  key <- c("date","machine_name")
  use_date <- F
  if(!is.null(query)){
    q <- copy(as.data.table(query))
    checkColumns(key, q)
    query_date <- q[, dateStrToPosix(date, tz="GMT")]
    q[, date := query_date]
    use_date <- T
    setkeyv(q,key)
  }
  
  if(is.null(index_file)){
	  if(use_cached)
		all_db_files <- list.files(result_dir,recursive=T, pattern="*\\.rdb$")
	  else
		all_db_files <- list.files(result_dir,recursive=T, pattern="*\\.db$")
	}
  else{
    all_db_files <- scan(index_file, what="character")
   }
  
	  fields <- strsplit(all_db_files,"/")
	  valid_files <- sapply(fields,length) == 4
	  
	  all_db_files <- all_db_files[valid_files]
	 
  
  
  invalids = fields[!valid_files]
  if(length(invalids) > 0){
    warning("There are some invalid files:")
    
    for(i in 1:length(invalids)){
      warning(paste(invalids[[i]]),sep='/')
    }
  }
  fields <- fields[valid_files]
  files_info <- do.call("rbind",fields)
  
  if(length(all_db_files) == 0){
    stop(sprintf("No .db files detected in the directory '%s'. Ensure it is not empty.",result_dir))
  }
  files_info <- do.call("rbind",fields)
  files_info <- as.data.table(files_info)
  setnames(files_info, c("machine_id", "machine_name", "date","file"))

  if(use_date)
    files_info[,date:=as.POSIXct(date, "%Y-%m-%d", tz="GMT")]
  else
    files_info[,date:=as.POSIXct(date, "%Y-%m-%d_%H-%M-%S", tz="GMT")]
  
  files_info[,path := paste(result_dir,all_db_files,sep="/")]
  setkeyv(files_info,key)
  
  if(is.null(query))
    return(files_info)
  files_info[,n:=.N,by=key(files_info)]
  unique_fi = files_info[,.SD[.N],by=key(files_info)]
  #unique_fi = unique(files_info,fromLast = T)
  
  out <- unique_fi[q]
  
  duplicated_queries <- unique(out[n>1,.(n, date,machine_name)])
  if(nrow(duplicated_queries) > 0){
    for( i in 1:nrow(duplicated_queries)){
      str <- "Several files(%i) in machine %s and date %s. Keeping last file. Use date and time if it is not intended.}"        
      str <- sprintf(str,duplicated_queries[i,n], duplicated_queries[i,machine_name],duplicated_queries[i,as.character(date)])
      warning(str)
    }
  }
  # we don't need the column 'n' any longer
  out[, n:=NULL]
  nas <- is.na(out[,path]) 
  if(any(nas)){
    out_nas <- out[nas,]
    for(i in 1:nrow(out_nas)){
      warning(sprintf("No result for machine_name == %s and date == %s. Omiting query",out_nas[i,machine_name],out_nas[i,date])) 
    }
  }
  out <- na.omit(out)
  setkeyv(out, union(key(out),colnames(q)))
  out
}


#' Retrieves  metadata from an ethoscope result file
#' 
#' This function is used to obtain metadata -- such as *time and date* of the experiment ,
#' *acquisition device*, *version of the software* and such--
#' embedded in a result file generated by the ethoscope platform.
#'
#' @param FILE the name of the input file.
#' @return A list containing fields for metadata entries
#' @examples
#' \dontrun{
#' FILE <- "ethoscope/014/E_014/2016-01-25_21-36-04/2016-01-25_21-36-04_014.db"
#' path <- getSampleDataPath(FILE)
#' out <- loadEthoscopeMetaData(path)
#' names(out)
#' }
#' @seealso todo
#' @export
loadEthoscopeMetaData <- function(FILE){
  con <- dbConnect(SQLite(), FILE, flags=SQLITE_RO)
  metadata <- dbGetQuery(con, "SELECT * FROM METADATA")
  dbDisconnect(con)
  v <- as.list(metadata$value)
  names(v) <- metadata$field
  #fixme explicitly GMT
  v$date_time <- as.POSIXct(as.integer(v$date_time),origin="1970-01-01",tz = "GMT")
  return(v)		
}

availableROIs <- function(FILE){
  con <- dbConnect(SQLite(), FILE, flags=SQLITE_RO)
  roi_map <- as.data.table(dbGetQuery(con, "SELECT * FROM ROI_MAP"))
  setkey(roi_map, roi_idx)
  
  available_rois  <- roi_map[ ,roi_idx]
  dbDisconnect(con)
  return(available_rois)
}

# we obtain data from one ROI and optionaly preanalyse it, by applying FUN.
parseOneROI <- function(i, master_table,min_time, max_time, reference_hour,verbose,FUN,columns=NULL,...){
  
  region_id <- master_table[i,region_id]
  experiment_id <- master_table[i,experiment_id]
  path <- master_table[i,path]
  if(verbose)
    cat(sprintf("Loading ROI number %i from:\n\t%s\n",region_id,path))
  
  if(tools::file_ext(path) == "db")
    loadingFUN <- loadOneROI
  else if(tools::file_ext(path) == "rdb")
    loadingFUN <- loadOneROICached
  else
    stop(sprintf("Unsuported file extention in %s",path))
    
    
  out <- loadingFUN(path,	region_id=region_id,
                    min_time = min_time,
                    max_time = max_time, 
                    reference_hour=reference_hour,columns=columns)
  
  if(is.null(out) || nrow(out) == 0){
    warning(sprintf("No data in ROI %i, from FILE %s. Skipping",region_id, path))
    return(NULL)
  }
  
  if(!is.null(FUN)){
    out <- FUN(out,...)
    if(is.null(out)){
      warning(sprintf("No data in ROI %i after running FUN, from FILE %s. Skipping",region_id, path))
      return(NULL)
    }
  }
  
  out[,experiment_id:=experiment_id]
  setkeyv(out,c("experiment_id","region_id"))
}

# a helper function to load data from a single region

loadOneROI <- function( FILE,  region_id, min_time=0, max_time=Inf,  reference_hour=NULL, columns = NULL){
  
  metadata <- loadEthoscopeMetaData(FILE)
  con <- dbConnect(SQLite(), FILE, flags=SQLITE_RO)
  var_map <- as.data.table(dbGetQuery(con, "SELECT * FROM VAR_MAP"))
  setkey(var_map, var_name)
  roi_map <- as.data.table(dbGetQuery(con, "SELECT * FROM ROI_MAP"))
  
  roi_row <- roi_map[roi_idx == region_id,]
  if(nrow(roi_row) == 0 ){
    warning(sprintf("ROI %i does not exist, skipping",region_id))
    return(NULL)
  }
  if(max_time == Inf)
    max_time_condition <- ""
  else
    max_time_condition <-  sprintf("AND t < %e", max_time * 1000) 
  
  min_time <- min_time * 1000 
  
  if(is.null(columns)){
    selected_cols = "*"
  }
  else{
    
    if("is_inferred" %in% var_map$var_name)
      columns <- unique(c(columns, "is_inferred"))
    
    if(any(!columns %in% var_map$var_name))
      stop(sprintf("Some of the requested columns are NOT available. Available columns are: %s",paste(var_map$var_name, collapse = " ")))
    
    selected_cols = paste(unique(c("t",columns)),collapse=", ")
    var_map <- var_map[columns]
    setkey(var_map, var_name)
  }
  
  sql_query <- sprintf("SELECT %s FROM ROI_%i WHERE t >= %e %s",selected_cols, region_id,min_time, max_time_condition )
  
  roi_dt <- as.data.table(dbGetQuery(con, sql_query))
  
  if("id" %in% colnames(roi_dt))
    roi_dt$id <- NULL
  roi_dt[, region_id := region_id]
  
  if(!is.null(reference_hour)){
    p <- metadata$date_time
    hour_start <- as.numeric(format(p, "%H")) + as.numeric(format(p, "%M")) / 60 +  as.numeric(format(p, "%S")) / 3600
    ms_after_ref <- ((hour_start - reference_hour) %% 24) * 3600 * 1000
    roi_dt[, t:= (t + ms_after_ref) ]
  }
  
  #time_in_seconds
  roi_dt[, t:= t/1e3]
  
  roi_width <- max(c(roi_row[,w], roi_row[,h]))
  for(var_n in var_map$var_name){
    if(var_map[var_n, functional_type] == "distance"){
      roi_dt[, (var_n) := get(var_n) / roi_width]
    }
    if(var_map[var_n, sql_type] == "BOOLEAN"){
      roi_dt[, (var_n) := as.logical(get(var_n))]
    }
  }
  if("is_inferred" %in% colnames(roi_dt))
    roi_dt <- roi_dt[is_inferred==F]
  roi_dt$is_inferred <- NULL
  return(roi_dt)
}

NULL

# A function to load data from a .rdb file this is faster cause it is indexed and R binary data.
# See cacheEthoscopeData in order to generate cached files
loadOneROICached <- function( FILE,  region_id, min_time=0, max_time=Inf,  reference_hour=NULL, columns = NULL){
  
  lazyLoad(tools::file_path_sans_ext(FILE))
  
  metadata <- METADATA
  var_map <- VAR_MAP
  setkey(var_map, var_name)
  roi_map <- ROI_MAP
  
  roi_row <- roi_map[roi_idx == region_id,]
  if(nrow(roi_row) == 0 ){
    warning(sprintf("ROI %i does not exist, skipping",region_id))
    return(NULL)
  }
  
  roi_dt <- eval(as.symbol(sprintf("ROI_%i",region_id)))
  roi_dt[,region_id := region_id]
  
  if(!is.null(reference_hour)){
    p <- as.POSIXct(as.numeric(metadata[field == "date_time",value]), origin="1970-01-01", tz= "GMT")
    hour_start <- as.numeric(format(p, "%H")) + as.numeric(format(p, "%M")) / 60 +  as.numeric(format(p, "%S")) / 3600
    ms_after_ref <- ((hour_start - reference_hour) %% 24) * 3600 * 1000
    roi_dt[, t:= (t + ms_after_ref) ]
  }
  
  roi_width <- max(c(roi_row[,w], roi_row[,h]))
  
  if(!is.null(columns)){
    columns_to_remove <- intersect(var_map$var_name[!var_map$var_name %in% columns], colnames(roi_dt))
    columns_to_remove <- c(columns_to_remove,"id")
  }
  else
    columns_to_remove <- "id"
  
  roi_dt[, (columns_to_remove) := NULL]
  
  for(var_n in intersect(var_map$var_name,colnames(roi_dt))){
    if(var_map[var_n, functional_type] == "distance"){
      roi_dt[, (var_n) := get(var_n) / roi_width]
    }
    else if(var_map[var_n, sql_type] == "BOOLEAN"){
      roi_dt[, (var_n) := as.logical(get(var_n))]
    }
  }
  
  roi_dt[, t:= t/1e3]
  if(min_time > 0 | is.finite(max_time))
    roi_dt <- roi_dt[between(t, min_time,max_time)]
  return(roi_dt)
}

NULL
#' Caches incrementally db  files to R native files
#' 
#' This function is meant to be run by heavy users who want to speed up reading ethoscope file.
#' It will essentially preload all data in a result directory into a cached directory with the same structure.
#' .rdb and .idx fils are generated instead of .db file.
#' In practice, this function will be run every day to fetch and cache new data. 
#' @param result_dir The location of the result directory (i.e. the folder containing all the data).
#' @param cached_dir The location of the directory where data should be saved.
#' @param dry_run The location of the directory where data should be saved.
#' @export
cacheEthoscopeData <- function(result_dir, cached_dir, dry_run=F){
  db_files <- buildEthoscopeQuery(result_dir)
  db_files[,cached_dir := sprintf("%s/%s/%s/%s",
                                  cached_dir,
                                  machine_id,
                                  machine_name,
                                  format(date,"%Y-%m-%d_%H-%M-%S")
  )]
  checkDirExists(result_dir)
  checkDirExists(cached_dir)
  
  db_files[,cached_path_idx := paste0(cached_dir,"/",basename(file_path_sans_ext(path)),".rdx")]
  db_files[,db_mtime := file.info(path)["mtime"]]
  db_files[,idx_mtime := file.info(cached_path_idx)["mtime"]]
  db_files[,needs_building := is.na(idx_mtime) | db_mtime > idx_mtime]
  db_files[,order(file)]
  if(dry_run)
    return(db_files)
  
  db_files <- db_files[needs_building==T]
  print(sprintf("Building %i files",nrow(db_files)))
  lapply(1:nrow(db_files),function(i){
    print(i)
    db_files[i,buildCacheDir(path, cached_dir)]
  })
}


sqliteTableToDataTable <- function(name,connection, dt, rm_inferred){
  dt <- dbGetQuery(connection, sprintf("SELECT * FROM %s",name))
  dt <- as.data.table(dt)
  if(rm_inferred & "is_inferred" %in% colnames(dt)){
    dt <- dt[is_inferred == FALSE]
    dt[,is_inferred := NULL]
  }
  dt
}

sqliteToRdb <- function(input_db, output_rdb,rm_inferred=TRUE){
  con <- dbConnect(SQLite(), input_db, flags=SQLITE_RO)
  
  list_of_table <- dbGetQuery(con,"SELECT name FROM sqlite_master WHERE type='table'")$name
  dt_list <- lapply(list_of_table, sqliteTableToDataTable,con, rm_inferred=rm_inferred)
  names(dt_list) <- list_of_table
  rdata_file <- sprintf("%s.RData",output_rdb)
  en <- list2env(dt_list)
  save(list=names(dt_list),envir = en, file=rdata_file)
  
  
  en <- local({load(rdata_file); environment()})  
  makeLazyLoadDB <- getFromNamespace(x = "makeLazyLoadDB", "tools")
  makeLazyLoadDB(en, output_rdb, compress=FALSE)
  file.remove(rdata_file)
}


buildCacheDir <- function(input_db, output_dir){
  
  print(sprintf("building %s, from %s", output_dir, input_db))
  session_tmp_dir <- tempdir()
  tmp_dir <- paste0(session_tmp_dir,"/",output_dir)
  out_file_name <- paste0(tmp_dir,"/",file_path_sans_ext(basename(input_db)))
  print(tmp_dir)
  dir.create(tmp_dir, recursive = TRUE, showWarnings = FALSE)
  
  return_value <- TRUE
  
  tryCatch(
    { 
      sqliteToRdb(input_db, out_file_name)
      dir.create(output_dir, recursive = TRUE,showWarnings = F)
      cached_files <-  list.files(tmp_dir, full.names = T)
      file.copy(cached_files, output_dir, overwrite=T)
    },
    error=function(e){return_value<-FALSE},
    finally= unlink(tmp_dir,recursive = T)
  )
  if(dir.exists(tmp_dir)){
    stop("dir not removed")
  }
  # only after we are sure that all is created do we move the file
  
  #dirname(output_dir)
  return(return_value)
}
