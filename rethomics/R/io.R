#@include
NULL
#' Read data from a result file.
#' 
#' This function is used to convert all the information
#' contained in a result file generated by PSV (i.e a .db file) into an R `data.table'.
#'
#' @param what an object describing which file(s) to load and, optionally, associated variables/conditions (see details).
#' @param min_time exclude data before min_time (in seconds). This time is relative to the start of the experiement.
#' @param max_time exclude data after max_time (in seconds). Also relative to the start of the experiement.
#' @param reference_hour the hour, in the day, to use as t_0 reference. When unspecified, time in the output is relative to the start of the experiment.
#' @param verbose whether to print progress (a logical).
#' @param FUN an optionnal function to transform the data from each `region' (i.e. a data.table) immedidatly after is has been loaded. 
#' @param ... extra arguments to be passed to \code{FUN}
#' @return A data.table where every row is an individual measurment. That is a position at a unique time (\code{t}) in a 
#' unique region (\code{region_id}), and from a unique result file/experiment (\code{experiment_id}).
#' The time is expressed in seconds. Distance units (e.g. xy position, height/width) are expressed as a fraction of the with of the region they originate from.
#' @details \code{what} can be one of two objects:
#' \itemize{
#'  \item{A character vector. }{In which case, it is assumed that each element is the path to a different file to load.}
#'  \item{A dataframe. }{The dataframe \emph{must} have a column named `path`. 
#'  The path basename will be used as a unique identifier for a specific experiement (\code{experiment_id}).
#' Arbitrary column can be added to map experimental condition to file name.
#' In addition, the dataframe can have a column named \code{region_id}. When defined, only the specified combinations of \code{path} and \code{region_id}
#' will be loaded. This allows to map additionnal conditions (data frame columns) to specific regions/files.
#' When additionnal conditions are provided, they will result in creation of custom columns in the output of this function.}
#' }
#'
#' @examples
#'
#' # First of all, let us load files from the data sample with the package.
#' # Most likely, you will already have your own data files.
#' sample_files <- c("tube_monitor_validation_subset.db",
#'                   "monitor_validation_subset.db")
#' # Extract the files in your computer
#' paths <- sapply(sample_files, loadSampleData)
#' # Now, `paths` is just a vector of file names:
#' print(paths)
#' #################
#' #################
#' # Case 1: load ALL REGIONS from a SINGLE FILE
#' validation_data_file <- paths[1]
#' # `validation_data_file` is simply the path to the db file in your computer
#' dt <- loadPsvData(validation_data_file)
#' print(dt)
#' ###############
#' # Case 2: load ALL REGIONS from MULTIPLE FILES
#' # we pass all the files we want to load as thw `what` argument
#' dt <- loadPsvData(paths)
#' # Note the column `experiment_id` in dt. It tells us which file/experiement 
#' # each measurment originates from.
#' print(dt)
#'
#' ###############
#' # Case 3: load ALL ROIS from MULTIPLE FILES AND add CONDITIONS
#' # Let us imagine that each file/experiement
#' # was acquired under different experiemental condition.
#' # We can encode this information in a 'master-table' (i.e a data.frame) 
#' # in which a column named \code{path} maps experiemental condition(s). 
#' #For instance 3 different treatments:
#' master_table <- data.frame(path=paths, treatment=c("control", "drug_A"))
#' # Let us check our table:
#' print(master_table)
#' # The table looks OK, we load the actual data
#' dt <- loadPsvData(master_table)
#' # Note that `dt` now contains a column for your treatment.
#' print(colnames(dt))
#' # This makes it easier to perform things such as average per treatment.
#' print(dt[,.(mean_x = mean(x)),by="treatment"])
#' ###############
#' # Case 4: load SELECTED REGIONS from MULTIPLE FILE, WITH CONDITIONS
#' # Sometimes, different regions contain for different conditions.
#' # If the master table has a column named `region_id`, 
#' # only the specified regions will be returned.
#' # Let us assume that we want to replicate case 3, 
#' # but, now, we load only the first 20 regions.
#' master_table <- data.table(path=paths, 
#'                            treatment=c("control", "drug_A"), 
#'                            region_id=rep(1:20,each= 2))
#' # We could also imagine that every even region is a male,
#' # and every odd is a female:
#' master_table[, sex := ifelse(region_id %% 2, "male", "female" )]
#' # Note that we have now two conditions.
#' # Let us check our new table:
#' print(master_table)
#' # Then we can load our data:
#' dt <- loadPsvData(master_table)
#' # This is simply a subset of data, so many regions are missing
#' # lets display the regions we ended up with
#' print(dt[,.(NA),by=key(dt)])
#' ####################
#' # Case 5: Apply ANALYSIS/function while loading the data.
#' # You can also apply a function from this package,
#' # or your own function to the data as it is being loaded.
#' # For instance, if you wish to peform a "sleep annotation":
#' dt <- loadPsvData(paths[1], FUN=sleepAnnotation)
#' # You could of course combine this with more conditions/region selection.
#' # For most complicated cases, you would have probably generated the 
#' # master-table (e.g. as a csv file) before analysing the results.

#' @seealso \code{\link{loadMetaData}} To display global informations about a specific file. 
#' @export
loadPsvData <- function(what,
				min_time = 0,
				max_time = Inf, 
				reference_hour=NULL,
				verbose=TRUE,
				FUN=NULL,
				...){
	
	# from the `what` argument, we build a `master_table` that we will map to the actual data.
	
	# case 1 what is a file, or a vector of files
	if(is.character(what)){
		# todo check whether file exists
		master_table <- data.table(path=what, experiment_id=basename(what))
		# We load all available ROIs since user did not provide ROI info
		master_table <- master_table[,list(
				region_id=availableROIs(path),
				experiment_id=experiment_id),by=path]
	}
	else if(is.data.frame(what)){
		
		if(!"path" %in% colnames(what))
			stop("When `what` is a data.frame, it MUST have a column named 'path'")
		master_table <- as.data.table(what)
		#fixme check uniqueness of file/use path as key?
		master_table[,path := as.character(path)]
		master_table[,experiment_id := basename(path)]
		
		setkey(master_table,experiment_id)
		
		if(!"region_id" %in% colnames(what)){
			m <- master_table[,list(region_id=availableROIs(path)),by=key(master_table)]
			master_table <- m[master_table]
		}

		}
	else{
		stop("Unexpected `what` argument!")
		}
	
	setkeyv(master_table,c("experiment_id","region_id"))

	l_dt <- lapply(1:nrow(master_table),
			function(i){
			  region_id <- master_table[i,region_id]
			  experiment_id <- master_table[i,experiment_id]
				path <- master_table[i,path]
				if(verbose)
					cat(sprintf("Loading ROI number %i from:\n\t%s\n",region_id,path))
					
				out <- loadOneROI(path,	region_id=region_id,
									min_time = min_time,
									max_time = max_time, 
									reference_hour=reference_hour)
				
				if(is.null(out) || nrow(out) == 0){
					warning(sprintf("No data in ROI %i, from FILE %s. Skipping",region_id, path))
					return(NULL)
					}
					
				if(!is.null(FUN)){
					out <- FUN(out,...)
					if(is.null(out)){
					  warning(sprintf("No data in ROI %i after running FUN, from FILE %s. Skipping",region_id, path))
					  return(NULL)
					}
				}
				
				out[,experiment_id:=experiment_id]
				setkeyv(out,c("experiment_id","region_id"))
				})
				
	l_dt <- l_dt[!sapply(l_dt,is.null)]
	if(length(unique(lapply(l_dt,key))) > 1){
		stop("Data tables do not have the same keys")
		}
 
	keys <- key(l_dt[[1]])

	out <- rbindlist(l_dt)
	rm(l_dt)
	
	setkeyv(out, keys)
	master_table[out]
}

# a helper function to laod data from a single region
loadOneROI <- function(
		FILE,
		region_id, 
		min_time=0, # In  s
		max_time=Inf,
		reference_hour=NULL){
		
		metadata <- loadMetaData(FILE)
		con <- dbConnect(SQLite(), FILE)
		
		var_map <- as.data.table(dbGetQuery(con, "SELECT * FROM VAR_MAP"))
		setkey(var_map, var_name)
		roi_map <- as.data.table(dbGetQuery(con, "SELECT * FROM ROI_MAP"))
		roi_row <- roi_map[roi_idx == region_id,]
		if(nrow(roi_row) == 0 ){
			warning(sprintf("ROI %i does not exist, skipping",region_id))
			return(NULL)
		}
		if(max_time == Inf)
			max_time_condition <- ""
		else
			max_time_condition <-  sprintf("AND t < %e", max_time * 1000) 
			
		min_time <- min_time * 1000 
		
		sql_query <- sprintf("SELECT * FROM ROI_%i WHERE t >= %e %s",region_id,min_time, max_time_condition )
		
		roi_dt <- as.data.table(dbGetQuery(con, sql_query))
		roi_dt[, id := NULL]
		roi_dt[, region_id := region_id]
		
		
		if(!is.null(reference_hour)){
			p <- metadata$date_time
			hour_start <- as.numeric(format(p, "%H")) + as.numeric(format(p, "%M")) / 60 +  as.numeric(format(p, "%S")) / 3600
			ms_after_ref <- ((hour_start - reference_hour) %% 24) * 3600 * 1000
			roi_dt[, t:= (t + ms_after_ref) ]
		}
		
		#time_in_seconds
		roi_dt[, t:= t/1e3]
		
		roi_width <- max(c(roi_row[,w], roi_row[,h]))
		for(var_n in var_map$var_name){
			if(var_map[var_n, functional_type] == "distance"){
				roi_dt[, (var_n) := get(var_n) / roi_width]
			}
			if(var_map[var_n, sql_type] == "BOOLEAN"){
				roi_dt[, (var_n) := as.logical(get(var_n))]
			}
		}
		if("is_inferred" %in% colnames(roi_dt))
			roi_dt <- roi_dt[is_inferred==F]
			roi_dt$is_inferred <- NULL
			
		return(roi_dt)
	}

NULL

#' Retreive metadata from a result file.
#' 
#' This function is used to obtain metadata -- such as `time and date of the experiment' , `acquisition device', `version of the software' and such--
#' embeded in a result file generated by PSV.
#'
#' @param FILE the name of the input file.
#' @return A list containing fields for metadata entries
#' @examples
#' \dontrun{
#' FILE <- "result.db"
#' out <- loadMetaData(FILE)
#' names(out)
#' }
#' @seealso \code{\link{loadPsvData}} to load raw data. 
#' @export
loadMetaData <- function(FILE){
	con <- dbConnect(SQLite(), FILE)
	metadata <- dbGetQuery(con, "SELECT * FROM METADATA")
	dbDisconnect(con)
	v <- as.list(metadata$value)
	names(v) <- metadata$field
	#fixme explicitly GMT
	v$date_time <- as.POSIXct(as.integer(v$date_time),origin="1970-01-01",tz = "GMT")
	return(v)		
	}
	
# @include
NULL
#' Read a text file formatted as DAM2 into a single data table.
#'
#' This function is used to load data from DAM2 devices as a data.table.
#'
#' @param FILE the name of the input file.
#' @param min_time exclude data before min_time (in seconds). This time is relative to the start of the experiement.
#' @param max_time exclude data after max_time (in seconds). This time is relative to the start of the experiement.
#' @return If \code{rois} has only one element, a dataframe. Otherwise, a list of dataframes (one per ROI)
#' @note Analysis of many long (sevaral days) recording can use a large amount of RAM.
#' Therefore, it can sometimes be advantageaous to load an process ROIs one by one.
#' @examples
#' \dontrun{
#' FILE <- "Monitor53.txt"
#' out <- loadDAMFile(FILE)
#' #histogram of x marginal distribution
#' hist(out[roi_id == 1, x], nclass=100)
#' }
#' \dontrun{
#' # More realistec example where we have experiemental conditions, and
#' we want to resample data at 1.0Hz.
#' # First, the conditions:
#' conditions <- cbind(roi_id=1:32, expand.grid(treatment=c(T,F), genotype=LETTERS[1:4]))
#' print(conditions)
#' }
#' @seealso \code{\link{loadMetaData}} To display global informations about the experiment.
#' @export
loadDAMFile <- function(FILE, channels = NULL, min_time = 0, max_time = Inf, interval = 60){
	### hardcodded constants
	DAM_COL_TYPES <- c(
		"integer", "character", "character",
		#rep(NULL, 7), ## these columns are irrelevant, NULL means we will discard them straight away
		rep("double", 32)
		)

	DAM_COL_NAMES <- c("idx", "day_month_year", "time", sprintf("channel_%02d", 1:32))

	dt_list <- fread(FILE, drop=4:10, header = FALSE)
	setnames(dt_list,DAM_COL_NAMES)
	dt_list[,datetime:=paste(day_month_year,time, sep=":")]
	dt_list[,t:=as.POSIXct(strptime(datetime,"%d %b %Y:%H:%M:%S"))]
	#clean table from unused parameters (idx,time, datetime...)
	dt_list[,time:=NULL]
	dt_list[,datetime:=NULL]
	dt_list[,idx:=NULL]
	dt_list[,day_month_year:=NULL]
	#for (i in seq(1,7,1) dt_list[,not_in_use_+i]
	#melting using pacakage reshape
	dt_risonno <- as.data.table(melt(dt_list,id="t"))

    roi_value <- function(channel_string){
        s <- strsplit(channel_string,"_")
        num <- as.integer(sapply(s,function(x) x[2]))
        return(num)
    }
    #get the values on activity
    dt_risonno[,activity:=value]
    dt_risonno[,roi_id:=roi_value(as.character(variable))]
	return(dt_risonno)
}


NULL

loadDAMFiles <- function(FILES, channels = NULL, min_time = 0, max_time = Inf, interval = 60){
	### hardcodded constants
	DAM_COL_TYPES <- c(
		c("integer", "character", "character", "character", "character"),
		rep("NULL", 7), ## these columns are irrelevant, NULL means we will discard them straight away
		rep("double", 32)
		)

	DAM_COL_NAMES <- c("idx", "day", "month", "year", "time",rep(NA, 7), sprintf("channel_%02d", 1:32))
	
	# todo sort files per actual dates before concat
	df_list <- lapply(FILES, function(f){
			read.table(f, colClasses = DAM_COL_TYPES, header = FALSE, col.names=DAM_COL_NAMES)
		})

	df <- do.call("rbind", df_list)
	#quick fix
	df$t <- 1:nrow(df) * interval #(s)
	df$day <- NULL
	df$month <- NULL
	df$year <- NULL
	df$time <- NULL
	
	if(is.null(channels))
		channels <- 1:32
	
	df <- subset(df, t > min_time & t < max_time)
	chans_to_fecth <- sprintf("channel_%02d", channels)
	
	df_l <- lapply(chans_to_fecth, function(x){
			data.frame(t=df$t,activity=df[,x])
		})
	names(df_l) <- chans_to_fecth	
	return(df_l)
}	

NULL
#' Retreive sample/example data contained in this package.
#' 
#' This function is intended to be used in order to try and test this package with a reproducible set of raw db data.
#' @param names The name of the samples to be loaded. When \code{names} is \code{NULL}, the function returns the list of available samples.
#' @seealso \code{\link{loadPsvData}} to obtain raw experimental data. 
#' @export
loadSampleData <- function(names=NULL){
  db_file <- system.file("data/db_files.tar.xz", package="rethomics")
  
  if(is.null(names)){
		content <- untar(db_file, list=T)
		db_files <- content
		out <- basename(content)[dirname(content) != '.']
		return(out)
		}

	d <- tempdir()
	file_name <- file.path("db_files",names)
	r <- untar(db_file, files=file_name,exdir=d)
	if(r == 2){
		unlink(d, recursive=T)
		stop("INVALID FILE NAME. List available files using `list=TRUE`")
		}
	out <-file.path(d,file_name)
	warning("Do not, forget to unlink file")
	return(out)
}
NULL
availableROIs <- function(FILE){
	con <- dbConnect(SQLite(), FILE)
	roi_map <- as.data.table(dbGetQuery(con, "SELECT * FROM ROI_MAP"))
	setkey(roi_map, roi_idx)

	available_rois  <- roi_map[ ,roi_idx]
	dbDisconnect(con)
	return(available_rois)
}
NULL
#'  Query files from a PSV data directory according to the date of the experiment and the device which acquired the data.
#' 
#' This function is designed to list and select experiemental files. In general, end-users will want to retreive path to their experimental files
#' according to the date and ID of the video monitor without having to undertand the underlying directory structure.
#' @param result_dir The location of the result directory (i.e. the folder containing all data).
#' @param query An optionnal query formated as a dataframe (see details).
#' @return
#' The query extended with the requested paths. When \code{query} is not specified, the function returns a table with all available files.
#' @details
#' The optionnal argument \code{query} is expected to be a table where every row maps an experiment.
#' In many respects, it is similar to the \code{what} argument in \code{\link{loadPsvData}}.
#' The only difference is that it does not have a \code{path} column. Instead, it must contain two columns:
#' \itemize{
#'  \item{\code{time} }{The date and time when the experiment started formated either as `yyyy-mm-dd' or `yyyy-mm-dd_hh:mm:ss'.
#'  In the former case, there may be several matching experiments to a single time (starting the same day).
#'  When this happends, \emph{only the last} is returned, and a warning message is displayed.}
#'  \item{\code{machine_name} }{The name of the machine that acquired the data.}
#'}
#' The result is meant to be used directly, as the \code{what} argument, by \code{\link{loadPsvData}} (see examples).
#' @note
#' PSV stores the data in a hard-coded directory structure\code{/root_dir/machine_id/machine_name/datetime/file.db}:
#' \itemize{
#'  \item{\code{machine_id} }{In principle, a universally unique identifier of the acquisition device.}
#'  \item{\code{machine_name} }{A human friendly name for acquisition device. In practice, this is expected to be unique within laboratory.}
#'  \item{\code{datetime} }{The date (and optionnally the time) of the begining of an experiment}
#' }
#' @examples
#' \dontrun{
#' # This is where I store the data on my computer
#' MY_DATA_DIR <- "/data/psv_results/"
#' 
#' query <- data.table(date="2015-06-02", machine_name=c("GGSM-001","GGSM-003"),region_id = rep(1:10,each=2))
#' print(query)
#' map <- fetchPsvResultFiles(MY_DATA_DIR, query)
#' dt <- loadPsvData(map)
#' }
#' @export

fetchPsvResultFiles <- function(result_dir,query=NULL){
  key <- c("date","machine_name")
  use_date <- F
  if(!is.null(query)){
    q <- copy(query)  
    if(!("date" %in% colnames(q)))
      stop("Query MUST have a `date` column")
    
    t <- q[,as.POSIXct(date, "%Y-%m-%d_%H-%M-%S", tz="GMT")]
    if(any(is.na(t)))
    t <- q[,as.POSIXct(date, "%Y-%m-%d", tz="GMT")]
    use_date <- T
    
    q[,date :=t]
    setkeyv(q,key)
  }
  all_db_files <- list.files(result_dir,recursive=T, pattern="*.db")
  
  fields <- strsplit(all_db_files,"/")
  valid_files <- sapply(fields,length) == 4
  
  all_db_files <- all_db_files[valid_files]
  files_info <- do.call("rbind",fields[valid_files])
  files_info <- as.data.table(files_info)
  setnames(files_info, c("machine_id", "machine_name", "date","file"))
  if(use_date)
    files_info[,date:=as.POSIXct(date, "%Y-%m-%d", tz="GMT")]
  else
    files_info[,date:=as.POSIXct(date, "%Y-%m-%d_%H-%M-%S", tz="GMT")]
  
  files_info[,path := paste(result_dir,all_db_files,sep="/")]
  setkeyv(files_info,key)
  
  if(is.null(query))
    return(files_info)
  out <- files_info[q]
  setkeyv(out,colnames(q))
  dups_i <- duplicated(out,fromLast=T)
  dups <- out[dups_i]

  if(nrow(dups) > 0){
    str <- "Duplicated queries. Excluding the following files:"
    str_v <- sprintf("%s, %s",dups[,machine_name],dups[,date])
    str_e <- "The LATEST files were kept"
    warning(paste(c(str, str_v, str_e), sep="\n"))
  }
  cols <- unique(c("path",key(out),colnames(q)))
  out <- out[!dups_i,cols,with=F]
  
  nas <- is.na(out[,path]) 
  if(any(nas)){
    out_nas <- out[nas,]
    for(i in nrow(out_nas)){
      warning(sprintf("No result for machine_name == %s and date == %s. Omiting query",out_nas[i,machine_name],out_nas[i,date])) 
    }
  }
  na.omit(out)
}
