#@include
NULL
#' Read data from a result file.
#' 
#' This function is used to convert all the information
#' contained in a result file generated by PSV (i.e a .db file) into an R `data.table'.
#'
#' @param what an object describing which file(s) to load and, optionally, associated variables/conditions (see details).
#' @param min_time exclude data before min_time (in seconds). This time is relative to the start of the experiment.
#' @param max_time exclude data after max_time (in seconds). It is also relative to the start of the experiment.
#' @param reference_hour the hour, in the day, to use as t_0 reference. When unspecified, time will be relative to the start of the experiment.
#' @param verbose whether to print progress (a logical).
#' @param cache_files whether SQL files should be cached in a tmp dir for faster reading
#' @param n_cores how many cores should be used to read/convert data
#' @param FUN an optional function to transform the data from each `region' (i.e. a data.table) immediately after is has been loaded. 
#' @param ... extra arguments to be passed to \code{FUN}
#' @return A data.table where every row is an individual measurement. That is a position at a unique time (\code{t}) in a 
#' unique region (\code{region_id}), and from a unique result file/experiment (\code{experiment_id}).
#' The time is expressed in seconds. Distance units (e.g. xy position, height/width) are expressed as a fraction of the width of the region they originate from.
#' @details \code{what} can be one of two objects:
#' \itemize{
#'  \item{A character vector. }{In which case, it is assumed that each element is the path to a different file to load.}
#'  \item{A dataframe. }{The dataframe \emph{must} have a column named `path'. 
#'  The path basename will be used as a unique identifier for a specific experiment (\code{experiment_id}).
#' Arbitrary column can be added to map experimental conditions to file name.
#' In addition, the dataframe can have a column named \code{region_id}. When defined, only the specified combinations of \code{path} and \code{region_id}
#' will be loaded. This allows to map additional conditions (i.e. data frame columns) to specific regions/files.
#' When additional conditions are provided, they will result in creation of custom columns in the output of this function.}
#' }
#'
#' @examples
#'
#' # First of all, let us load files from the data sample included within this package.
#' # Most likely, you will already have your own data files.
#' sample_files <- c("tube_monitor_validation_subset.db",
#'                   "monitor_validation_subset.db")
#' # Extract the files in your computer
#' paths <- sapply(sample_files, loadSampleData)
#' # Now, `paths` is just a vector of file names:
#' print(paths)
#' #################
#' #################
#' # Case 1: load ALL REGIONS from a SINGLE FILE
#' validation_data_file <- paths[1]
#' # `validation_data_file` is simply the path to the .db file in your computer
#' dt <- loadPsvData(validation_data_file)
#' print(dt)
#' ###############
#' # Case 2: load ALL REGIONS from MULTIPLE FILES
#' # we pass all the files we want to load as the `what` argument
#' dt <- loadPsvData(paths)
#' # Note the column `experiment_id` in dt. It tells us which file/experiment 
#' # each measurement originates from.
#' print(dt)
#'
#' ###############
#' # Case 3: load ALL REGIONS from MULTIPLE FILES AND add CONDITIONS
#' # Let us imagine that each file/experiment
#' # was acquired under different experimental condition.
#' # We can encode this information in a 'master-table' (i.e a data.frame) 
#' # in which a column named \code{path} maps experimental condition(s). 
#' # For instance, 2 different treatments:
#' master_table <- data.frame(path=paths, treatment=c("control", "drug_A"))
#' # Let us check our table:
#' print(master_table)
#' # The table looks OK, so we load the actual data
#' dt <- loadPsvData(master_table)
#' # Note that `dt` now contains a column for your treatment.
#' print(colnames(dt))
#' # This makes it easier to perform things such as average per treatment.
#' print(dt[,.(mean_x = mean(x)),by="treatment"])
#' ###############
#' # Case 4: load SELECTED REGIONS from MULTIPLE FILE, WITH CONDITIONS
#' # Sometimes, different regions contain different conditions.
#' # If the master table has a column named `region_id`, 
#' # only the specified regions will be returned.
#' # Let us assume that we want to replicate case 3, 
#' # but, now, we load only the first 20 regions.
#' master_table <- data.table(path=paths, 
#'                            treatment=c("control", "drug_A"), 
#'                            region_id=rep(1:20,each= 2))
#' # We could also imagine that every even region contains a male,
#' # whilst every odd one has a female:
#' master_table[, sex := ifelse(region_id %% 2, "male", "female" )]
#' # Note that we have now two conditions.
#' # Let us check our new table:
#' print(master_table)
#' # Then we can load our data:
#' dt <- loadPsvData(master_table)
#' # This is simply a subset of data, so many regions are missing
#' # lets display the regions we ended up with
#' print(dt[,.(NA),by=key(dt)])
#' ####################
#' # Case 5: Apply ANALYSIS/function whist loading the data.
#' # You can also apply a function from this package,
#' # or your own function to the data as it is being loaded.
#' # For instance, if you wish to peform a `sleep annotation':
#' dt <- loadPsvData(paths[1], FUN=sleepAnnotation)
#' # You could of course combine this with more conditions/region selection.
#' # For most complicated cases, you would probably have pre-generated the 
#' # master-table (e.g. as a csv file) before analysing the results.

#' @seealso \code{\link{loadMetaData}} To display global informations about a specific file. 
#' @export
loadPsvData <- function(what,
				min_time = 0,
				max_time = Inf, 
				reference_hour=NULL,
				verbose=TRUE,
				cache_files=TRUE,
				n_cores=1,
				FUN=NULL,
				...){
	
	# from the `what` argument, we build a `master_table` that we will map to the actual data.
	
	# case 1 what is a file, or a vector of files
	if(is.character(what)){
		# todo check whether file exists
		master_table <- data.table(path=what, experiment_id=basename(what))
		# We load all available ROIs since user did not provide ROI info
		master_table <- master_table[,list(
				region_id=availableROIs(path),
				experiment_id=experiment_id),by=path]
	}
	else if(is.data.frame(what)){
		if(!"path" %in% colnames(what))
			stop("When `what` is a data.frame, it MUST have a column named 'path'")
		master_table <- copy(as.data.table(what))
		#fixme check uniqueness of file/use path as key?
		master_table[,path := as.character(path)]
		master_table[,experiment_id := basename(path)]
		
		setkey(master_table,experiment_id)
		
		if(!"region_id" %in% colnames(what)){
			m <- master_table[,list(region_id=availableROIs(path)),by=key(master_table)]
			master_table <- m[master_table]
		}

		}
	else{
		stop("Unexpected `what` argument!")
		}
	
  if(cache_files)
    master_table[, path:= cacheResultFile(master_table[,path])]
  
	setkeyv(master_table,c("experiment_id","region_id"))
  
	if(n_cores == 1)
	  l_dt <- lapply(1:nrow(master_table),parseOneROI, master_table,min_time, max_time, reference_hour,verbose,FUN,...)
	else{
	  require(parallel)
	  #cl <- makeCluster(n_cores)
	  l_dt <- mclapply(1:nrow(master_table),parseOneROI, master_table,min_time, max_time, reference_hour,verbose,FUN,...,mc.cores = getOption("mc.cores", n_cores))
	  # stopCluster(cl)
	}
				
	l_dt <- l_dt[!sapply(l_dt,is.null)]
	if(length(unique(lapply(l_dt,key))) > 1){
		stop("Data tables do not have the same keys")
		}
 
	keys <- key(l_dt[[1]])

	out <- rbindlist(l_dt)
	rm(l_dt)
	
	setkeyv(out, keys)
	out <- master_table[out]
	out$path <-NULL
	master_table$path <-NULL
	master_table
	setkeyv(out, colnames(master_table))
	out
}

parseOneROI <- function(i, master_table,min_time, max_time, reference_hour,verbose,FUN,...){
  
  region_id <- master_table[i,region_id]
  experiment_id <- master_table[i,experiment_id]
  path <- master_table[i,path]
  if(verbose)
    cat(sprintf("Loading ROI number %i from:\n\t%s\n",region_id,path))
  
  out <- loadOneROI(path,	region_id=region_id,
                    min_time = min_time,
                    max_time = max_time, 
                    reference_hour=reference_hour)
  
  if(is.null(out) || nrow(out) == 0){
    warning(sprintf("No data in ROI %i, from FILE %s. Skipping",region_id, path))
    return(NULL)
  }
  
  if(!is.null(FUN)){
    out <- FUN(out,...)
    if(is.null(out)){
      warning(sprintf("No data in ROI %i after running FUN, from FILE %s. Skipping",region_id, path))
      return(NULL)
    }
  }
  
  out[,experiment_id:=experiment_id]
  setkeyv(out,c("experiment_id","region_id"))
}

# a helper function to laod data from a single region
loadOneROI <- function(
		FILE,
		region_id, 
		min_time=0, # In  s
		max_time=Inf,
		reference_hour=NULL){
		
		metadata <- loadMetaData(FILE)
		con <- dbConnect(SQLite(), FILE)
		
		var_map <- as.data.table(dbGetQuery(con, "SELECT * FROM VAR_MAP"))
		setkey(var_map, var_name)
		roi_map <- as.data.table(dbGetQuery(con, "SELECT * FROM ROI_MAP"))
		roi_row <- roi_map[roi_idx == region_id,]
		if(nrow(roi_row) == 0 ){
			warning(sprintf("ROI %i does not exist, skipping",region_id))
			return(NULL)
		}
		if(max_time == Inf)
			max_time_condition <- ""
		else
			max_time_condition <-  sprintf("AND t < %e", max_time * 1000) 
			
		min_time <- min_time * 1000 
		
		sql_query <- sprintf("SELECT * FROM ROI_%i WHERE t >= %e %s",region_id,min_time, max_time_condition )
		
		roi_dt <- as.data.table(dbGetQuery(con, sql_query))
		roi_dt$id <- NULL
		roi_dt[, region_id := region_id]
		
		
		if(!is.null(reference_hour)){
			p <- metadata$date_time
			hour_start <- as.numeric(format(p, "%H")) + as.numeric(format(p, "%M")) / 60 +  as.numeric(format(p, "%S")) / 3600
			ms_after_ref <- ((hour_start - reference_hour) %% 24) * 3600 * 1000
			roi_dt[, t:= (t + ms_after_ref) ]
		}
		
		#time_in_seconds
		roi_dt[, t:= t/1e3]
		
		roi_width <- max(c(roi_row[,w], roi_row[,h]))
		for(var_n in var_map$var_name){
			if(var_map[var_n, functional_type] == "distance"){
				roi_dt[, (var_n) := get(var_n) / roi_width]
			}
			if(var_map[var_n, sql_type] == "BOOLEAN"){
				roi_dt[, (var_n) := as.logical(get(var_n))]
			}
		}
		if("is_inferred" %in% colnames(roi_dt))
			roi_dt <- roi_dt[is_inferred==F]
			roi_dt$is_inferred <- NULL
			
		return(roi_dt)
	}

NULL

#' Retrieves  metadata from a result file.
#' 
#' This function is used to obtain metadata -- such as `time and date of the experiment' , `acquisition device', `version of the software' and such--
#' embedded in a result file generated by PSV.
#'
#' @param FILE the name of the input file.
#' @return A list containing fields for metadata entries
#' @examples
#' \dontrun{
#' FILE <- "result.db"
#' out <- loadMetaData(FILE)
#' names(out)
#' }
#' @seealso \code{\link{loadPsvData}} to load raw data. 
#' @export
loadMetaData <- function(FILE){
	con <- dbConnect(SQLite(), FILE)
	metadata <- dbGetQuery(con, "SELECT * FROM METADATA")
	dbDisconnect(con)
	v <- as.list(metadata$value)
	names(v) <- metadata$field
	#fixme explicitly GMT
	v$date_time <- as.POSIXct(as.integer(v$date_time),origin="1970-01-01",tz = "GMT")
	return(v)		
	}
	
# @include
NULL
#' Read a text file formatted as DAM2 into a single data table.
#'
#' This function is used to load data from DAM2 devices as a data.table.
#'
#' @param FILE the name of the input file.
#' @param start_date the starting date formated as "yyyy-mm-dd" or "yyyy-mm-dd_hh-mm-ss"
#' @param stop_date the last day of the experiment. Same format as \code{start_date}
#' @param tz the time zone of the computer saving the file. By default, \code{tz} is taken from the computer running this function
#' @param verbose whether to print progress (a logical).
#' @return a data table with an activity (number of beam crosses) variable, a region_id (channel) variable and a posix time stamp.
#' @examples
#' \dontrun{
#' FILE <- "Monitor53.txt"
#' out <- loadDAMFile(FILE)
#' #histogram of x marginal distribution
#' hist(out[roi_id == 1, x], nclass=100)
#' }
#' \dontrun{
#' # More realistic example where we have experimental conditions, and
#' we want to resample data at 1.0Hz.
#' # First, the conditions:
#' conditions <- cbind(roi_id=1:32, expand.grid(treatment=c(T,F), genotype=LETTERS[1:4]))
#' print(conditions)
#' }
#' @seealso \code{\link{loadMetaData}} To display global informations about the experiment.
loadDAMFile <- function(FILE, 
                        start_date=-Inf,
                        stop_date=+Inf,
                        tz = "",
                        verbose=TRUE
                        ){
  # 1 load time stamps
  if(verbose)
    print(sprintf("Reading %s.",FILE))
  dt <- fread(FILE, select=2:4, header = FALSE)
  dt[,datetime:=paste(V2,V3, sep=" ")]
  dt[,t:=as.POSIXct(strptime(datetime,"%d %b %y %H:%M:%S",tz=tz))]
  min_date <- dateStrToPosix(start_date,tz)
  max_date <- dateStrToPosix(stop_date,tz)
  
  # if time is not in date, we add a day
  # TODO
  # ifelse(parseDateStr(stop_date)$has_time, max_date,max_date +hours(24))
  
  if(max_date < min_date)
    stop("`max_date` MUST BE greater than `min_date`")
  valid_dt <- dt[,.(
    valid = (t >= min_date & t < max_date & V4 ==1),
    idx = 1:.N,
    t=t
  )]
  valid_dt <- valid_dt[valid == T]
  
  first = min(valid_dt[,idx]) 
  last = max(valid_dt[,idx])     
  
  #2 check time stamps
  if(nrow(valid_dt) < 1){
    stop("There is apparently no data in this range of dates")
  }
  
  valid_dt[,diff_t := c(NA,diff(t))]
  valid_dt <- na.omit(valid_dt)
  sampling_periods <- valid_dt[,.(n=.N),by=diff_t]
  if(nrow(sampling_periods) > 1){
    warning(sprintf("The sampling period is not always regular in %s.
            Some reads must have been skipped.",FILE))
    #fixme show a table of samplig rates
  }
  
  if(any(sampling_periods[,diff_t] == 0)){
    stop("Some of the dates are repeated between successive measument.
         This can happen if the recording computer change time (e.g. between winter and summer time)")
  }
  
  if(any(sampling_periods[,diff_t] < 0)){
    stop("Come measument appear to have been recorded 'before' previous measuments.
         It looks as if the recording computer went back in time!")
  }
  # 3 actually load the file
  DAM_COL_NAMES <- c("idx", "day_month_year", "time","status", sprintf("channel_%02d", 1:32))
  dt_list <- fread(FILE, drop=5:10, header = FALSE,
                   skip = first-1, nrows = last-first+1)
  setnames(dt_list,DAM_COL_NAMES)
  dt_list <- dt_list[status ==1]
  dt_list[,datetime:=paste(day_month_year,time, sep=" ")]
  dt_list[,t:=as.POSIXct(strptime(datetime,"%d %b %y %H:%M:%S",tz=tz))]
  #clean table from unused variables (idx,time, datetime...)
  dt_list[,time:=NULL]
  dt_list[,datetime:=NULL]
  dt_list[,idx:=NULL]
  dt_list[,day_month_year:=NULL]
  dt_list[,status:=NULL]
  
  out <- as.data.table(melt(dt_list,id="t"))
  
  roi_value <- function(channel_string){
    s <- strsplit(channel_string,"_")
    num <- as.integer(sapply(s,function(x) x[2]))
    return(num)
  }
  
  #get the values on activity
  out[,activity:=value]
  out[,region_id:=roi_value(as.character(variable))]
  out$value <- NULL
  out$variable <- NULL
  return(out)
}
NULL
#' Retrieves sample/example data contained within in this package.
#' 
#' This function is only for testing (and trying) purposes.  It provides a way to access raw data  (e.g. db files) contained within this package.
#' @param names The name of the samples to be loaded. When \code{names} is \code{NULL}, the function returns the list of all available samples.
#' @seealso \code{\link{loadPsvData}} to obtain raw experimental data. 
#' @export
loadSampleData <- function(names=NULL){
  db_file <- system.file("data/db_files.tar.xz", package="rethomics")
  
  if(is.null(names)){
		content <- untar(db_file, list=T)
		db_files <- content
		out <- basename(content)[dirname(content) != '.']
		return(out)
		}

	d <- tempdir()
	file_name <- file.path("db_files",names)
	r <- untar(db_file, files=file_name,exdir=d)
	if(r == 2){
		unlink(d, recursive=T)
		stop("INVALID FILE NAME. List available files using `list=TRUE`")
		}
	out <-file.path(d,file_name)
	warning("Do not, forget to unlink file")
	return(out)
}
NULL
availableROIs <- function(FILE){
	con <- dbConnect(SQLite(), FILE)
	roi_map <- as.data.table(dbGetQuery(con, "SELECT * FROM ROI_MAP"))
	setkey(roi_map, roi_idx)

	available_rois  <- roi_map[ ,roi_idx]
	dbDisconnect(con)
	return(available_rois)
}
NULL
#'  Query files from a PSV data directory according to the date of the experiment and the device which acquired the data.
#' 
#' This function is designed to list and select experimental files. In general, end-users will want to retrieve  path to their experimental files
#' according to the date and ID of the video monitor without having to understand the underlying directory structure.
#' @param result_dir The location of the result directory (i.e. the folder containing all the data).
#' @param query An optional query formatted as a dataframe (see details).
#' @return
#' The query extended with the requested paths. When \code{query} is not specified, the function returns a table with all available files.
#' @details
#' The optional argument \code{query} is expected to be a table where every row maps an experiment.
#' In many respects, it is similar to the \code{what} argument in \code{\link{loadPsvData}}.
#' The only difference is that it does not have a \code{path} column. Instead, it must contain two columns:
#' \itemize{
#'  \item{\code{date} }{The date and time when the experiment started formatted either as `yyyy-mm-dd' or `yyyy-mm-dd_hh:mm:ss'.
#'  In the former case, there may be several matching experiments to a single time (starting the same day).
#'  When this happens, \emph{only the last} is returned, and a warning message is displayed.}
#'  \item{\code{machine_name} }{The name of the machine that acquired the data.}
#'}
#' The result is meant to be used directly, as the \code{what} argument, by \code{\link{loadPsvData}} (see examples).
#' @note
#' PSV stores data in a hard-coded directory structure \code{/root_dir/machine_id/machine_name/datetime/file.db}, where:
#' \itemize{
#'  \item{\code{machine_id} }{Is, in principle, a universally unique identifier of the acquisition device.}
#'  \item{\code{machine_name}, }{a human friendly name for acquisition device. In practice, this is expected to be unique within laboratory.}
#'  \item{\code{datetime}, }{the date and time of the start of the experiment}
#' }
#' @examples
#' \dontrun{
#' # This is where I store the data on my computer
#' MY_DATA_DIR <- "/data/psv_results/"
#' 
#' query <- data.table(date="2015-06-02",
#'                     machine_name=c("GGSM-001","GGSM-003"),
#'                     region_id = rep(1:10,each=2))
#' print(query)
#' map <- fetchPsvResultFiles(MY_DATA_DIR, query)
#' dt <- loadPsvData(map)
#' }
#' @export

fetchPsvResultFiles <- function(result_dir,query=NULL){
  if(!dir.exists(result_dir)){
    stop("result_dir not found. Ensure you use the correct path")
  }
  key <- c("date","machine_name")
  use_date <- F
  if(!is.null(query)){
    q <- copy(as.data.table(query))
    q[, date:=as.character(date)]
    if(!("date" %in% colnames(q)))
      stop("Query MUST have a `date` column")
    
    t <- q[,as.POSIXct(date, "%Y-%m-%d_%H-%M-%S", tz="GMT")]
    if(any(is.na(t)))
      t <- q[,as.POSIXct(date, "%Y-%m-%d", tz="GMT")]
    use_date <- T
    
    q[,date :=t]
    setkeyv(q,key)
  }
  all_db_files <- list.files(result_dir,recursive=T, pattern="*\\.db$")
  
  
  fields <- strsplit(all_db_files,"/")
  valid_files <- sapply(fields,length) == 4
  
  all_db_files <- all_db_files[valid_files]
  
  if(length(all_db_files) == 0){
    stop(sprintf("No .db files detected in the directory '%s'. Ensure it is not empty.",result_dir))
  }
  
  files_info <- do.call("rbind",fields[valid_files])
  files_info <- as.data.table(files_info)
  setnames(files_info, c("machine_id", "machine_name", "date","file"))
  
  if(use_date)
    files_info[,date:=as.POSIXct(date, "%Y-%m-%d", tz="GMT")]
  else
    files_info[,date:=as.POSIXct(date, "%Y-%m-%d_%H-%M-%S", tz="GMT")]
  
  files_info[,path := paste(result_dir,all_db_files,sep="/")]
  setkeyv(files_info,key)
  
  if(is.null(query))
    return(files_info)
  files_info[,n:=.N,by=key(files_info)]
  unique_fi = unique(files_info,fromLast = T)
  out <- unique_fi[q]
  duplicated_queries <- unique(out[n>1,.(date,machine_name)])
  if(nrow(duplicated_queries) > 0){
    for( i in 1:nrow(duplicated_queries)){
      str <- "Duplicated queries. Excluding {%s, %s}"        
      str <- sprintf(str,duplicated_queries[i,machine_name],duplicated_queries[i,date])
      warning(str)
    }
  }
  nas <- is.na(out[,path]) 
  if(any(nas)){
    out_nas <- out[nas,]
    for(i in nrow(out_nas)){
      warning(sprintf("No result for machine_name == %s and date == %s. Omiting query",out_nas[i,machine_name],out_nas[i,date])) 
    }
  }
  out <- na.omit(out)
  setkeyv(out, union(key(out),colnames(q)))
  out
}
NULL
listDailyDAMFiles <- function(result_dir){
  fs <- list.files( result_dir,pattern="M...*\\.txt",recursive = T)
  fields <- strsplit(fs,"/")
  valid_files <- sapply(fields,length) == 4
  fs <- fs[valid_files]
  files_info <- do.call("rbind",fields[valid_files])
  files_info <- as.data.table(files_info)
  setnames(files_info, c("yyyy", "mm", "mmdd","file"))
  files_info[, machine_id := substr(file,5,8)]  
  files_info[, path := paste(result_dir,fs,sep="/")]
  files_info[, date := paste0(yyyy,mmdd)]
  files_info[, date:=as.POSIXct(date, "%Y%m%d", tz="GMT")]
  files_info$mm <- NULL
  files_info$yyyy<- NULL
  files_info$mmdd <- NULL
  files_info$file <- NULL
  files_info
}
#' Retrieves DAM2 data from daily saved files
#' 
#' Uses a query mechanism to get data from a DAM2 array. 
#' This is useful when data has been saved, by day, in individual files for each monitor.
#' 
#' @param result_dir the root directory where all daily data are saved
#' @param query a formatted query used to request data (see detail).
#' @param reference_hour the hour, in the day, to use as t_0 reference. This should be expressed on Greenwich Meridian Time.
#' @param tz the time zone on which the DAM2 data was saved (e.g. BSM -> British Summer Time)
#' @param FUN an optional function to transform the data from each `region' (i.e. a data.table) immediately after is has been loaded. 
#' @param verbose whether to print progress (a logical).
#' @param ... extra arguments to be passed to \code{FUN}
#' @return A data.table where every row is an individual measurement. That is an activity at a unique time (\code{t}) in a 
#' unique channel (\code{region_id}), and from a unique result date/experiment (\code{experiment_id}).
#' The time is expressed in seconds. For each different combination of \code{start_date} and \code{machine_id} in the query, an
#' individual \code{experiment_id} is generated.
#' @details \code{query} must be a data.table.
#' Conceptually, each row of the query describes the conditions in one channel (when \code{region_id} is specified), or in each monitor (when it is not).
#' It should have  the following columns:
#' \itemize{
#'  \item{\code{machine_id} }{ the name of the machine used (e.g. `M002').}
#'  \item{\code{start_date} }{ the first day of the requested experiment (e.g. `2014-12-28').}
#'  \item{\code{stop_date} }{ the last day of the requested experiment (e.g. `2014-12-30').}
#'  \item{\code{region_id} }{ the channel (between 1 and 32) in what the animal was in (e.g. `20'). This is an optional column. If not provided, all 32 channels are loaded with the same conditions.}
#'  \item{\code{...} }{ arbitrary columns to associate conditions/treatments/genotypes/... to the previous columns}
#' }
#'
#' @note
#' the daily data should be saved in a hard-coded directory structure \code{root_dir/yyyy/mm/mmdd/mmddMxyz.txt}, where:
#' \itemize{
#'  \item{\code{yyyy} }{Is the year (e.g. 2014)}
#'  \item{\code{mm} and \code{dd}, }{the formatted month and day, respectively (e.g. mm=12 and dd=28).}
#'  \item{\code{xyz}, }{the number of the monitor (e.g 003)}
#' }
#' @seealso \code{\link{queryDAMData}} to load data from a regular DAM2 file
#' @export
fetchDAMData <- function(result_dir,
                         query, 
                         reference_hour=9.0, 
                         tz="BST",
                         verbose=TRUE,
                         FUN=NULL, ...){
  q = copy(query)
  #files_info[, experiment_id := paste(date,machine_id,sep="_")]
  files_info <- listDailyDAMFiles(DAILY_DATA_DIR)
  
  if(!("region_id" %in% colnames(q)))
    q <- q[,.(region_id=1:32),by=c(colnames(q))]
  
  q[, start_date:=as.POSIXct(start_date, "%Y-%m-%d", tz="GMT")]
  q[, stop_date:=as.POSIXct(stop_date, "%Y-%m-%d", tz="GMT")]
  q[, experiment_id := paste(start_date,machine_id,sep="_")]
  
  foo <- function(d){
    t0 <- unique(d[,start_date])
    t1 <- unique(d[,stop_date])
    mid <- unique(d[,machine_id])
    eid <- unique(d[,experiment_id])
    out <- files_info[date >= t0 & date <= t1 & machine_id == mid, .(path)]
    
    out[,experiment_id := eid]
  }
  setkeyv(q,c("experiment_id"))
  
  uniq_q <- unique(q)
  uniq_q$region_id <- NULL
  setkeyv(uniq_q,c("experiment_id"))
  #setkeyv(uniq_q,c("experiment_id"))
  day_query <- uniq_q[,
                      foo(.SD)
                      ,by= 1:nrow(uniq_q)]
  day_query$nrow <- NULL
  setkeyv(day_query,c("experiment_id"))
  day_query <- uniq_q[day_query]
  
  bar <- function(files){
    out <- lapply(files, loadDAMFile,tz=tz,verbose=verbose)
    rbindlist(out)
  }
  
  all_data <- day_query[, 
                        bar(path)
                        ,by="experiment_id"]
  #all_data[,t:=as.numeric(t - min(t))]
  
  setkeyv(q,c("experiment_id","region_id"))
  setkeyv(all_data,c("experiment_id","region_id"))
  all_data <- all_data[q]
  
  q_key <- setdiff(colnames(q),c("start_date","stop_date"))
  setkeyv(all_data, union(key(all_data),q_key))
  all_data[,t := as.numeric(t  - start_date,units="secs")]
  all_data[,t := t-hours(reference_hour)]
  
  if(!is.null(FUN)){
    all_data <- all_data[, FUN(.SD,...),by=key(all_data)]
  }
  all_data
}

cacheResultFile <- function(all_paths,subdir="rethomic_file_cache"){
  src <- unique(all_paths)

  dst_dir <- paste(tempdir(),subdir,sep="/")
  if(!dir.exists(dst_dir))
    dir.create(dst_dir)
  dst <- paste(dst_dir,basename(src),sep="/")
  map <- data.table(src,dst,key="src")
  
  lapply(1:nrow(map),function(i){
    file.copy(map[i,src],map[i,dst],overwrite = F)
  })
  path_map <- data.table(src=all_paths)
  return(map[path_map][,dst])
}
NULL

#' Retrieves DAM2 data from continuous file recording
#' 
#' Uses a query mechanism to get data from a DAM2 array. 
#' This is usefuld when using the default behaviour of Trikinetics software
#' where data is simply appended to a single long file per monitor.
#' 
#' @param query a formatted query used to request data (see detail).
#' @param FUN an optional function to transform the data from each `region' (i.e. a data.table) immediately after is has been loaded. 
#' @param ... extra arguments to be passed to \code{FUN}
#' @return A data.table where every row is an individual measurement. That is an activity at a unique time (\code{t}) in a 
#' unique channel (\code{region_id}), and from a unique result date/experiment (\code{experiment_id}).
#' The time is expressed in seconds. For each different combination of \code{start_date} and \code{file} in the query, an
#' individual \code{experiment_id} is generated.
#' @details \code{query} must be a data.table.
#' Conceptually, each row of the query describes the conditions in one channel (when \code{region_id} is specified), or in each monitor (when it is not).
#' It should have  the following columns:
#' \itemize{
#'  \item{\code{path} }{ the location of your data file (e.g. `C:/User/me/Desktop/Monitor3.txt').}
#'  \item{\code{start_date} }{ the first day of the requested experiment (e.g. `2014-12-28').}
#'  \item{\code{stop_date} }{ the last day of the requested experiment (e.g. `2014-12-30').}
#'  \item{\code{region_id} }{ the channel (between 1 and 32) in what the animal was in (e.g. `20'). This is an optional column. If not provided, all 32 channels are loaded with the same conditions.}
#'  \item{\code{...} }{ arbitrary columns to associate conditions/treatments/genotypes/... to the previous columns}
#' }
#'
#' @examples
#' sample_file <- system.file('data/DAMfile.txt', package="rethomics")
#' query = data.table(path=sample_file,
#'                    # note the time (10:00) is added as reference time
#'                  start_date="2015-07-02_10-00-00", 
#'                  stop_date="2015-07-07",
#'                  region_id=c(1:32),condition=rep(letters[1:2],each=16))
#' print(query)
#' dt <- queryDAMFiles(query)
#' ethogramPlot(activity,dt,condition) + scale_x_continuous(breaks=0:10/2)
#' dt <- queryDAMFiles(query,FUN= sleepDAMAnnotation)
#' ethogramPlot(asleep,dt,condition) + scale_x_continuous(breaks=0:10/2)
#' @seealso \code{\link{fetchDAMData}} to load DAM data that is saved by day
#' @export
queryDAMFiles <- function(query, FUN=NULL, ...){
  tz=""
  q <- copy(as.data.table(query))
  cn <- colnames(q)
 if(!("path" %in% cn & "start_date" %in% cn & "stop_date" %in% cn )){
  stop("query MUST have at least thre columns names `path`, `start_date` and `stop_date`")
 }
  q[, experiment_id := paste(start_date,basename(path),sep="_")]
  if(!"region_id" %in% cn)
    q <- q[q[,.(region_id=1:32),by=experiment_id]]
  setkey(q,experiment_id)
  
  wrapLoadDAM <- function(p,srtd,stpd,tz){
    out <- loadDAMFile(p,srtd,stpd,tz=tz)
    out[,t:= t]
    out
  }
  
  out <- unique(q)[,
           wrapLoadDAM(path,start_date, stop_date,tz),
           by=experiment_id]
  
  q$path <- NULL
  setkeyv(out,c("experiment_id","region_id"))
  
  final_key <- copy(colnames(q))
  
  setkeyv(q,final_key)
  q[, t0:=sapply(start_date,dateStrToPosix,tz)]
  out <- merge(q,out)
  
  out[, t:=as.numeric(t-t0,units='secs')]
  out$t0 <- NULL
  q$t0 <- NULL
  
  setkeyv(out,final_key)
  
  if(!is.null(FUN)){
    out <- out[, FUN(.SD,...),by=key(out)]
  }
  
  setkeyv(out, union(key(out),colnames(q)))
  out
}
NULL


# valid_dates <- c('2015-05-04_10-00-00', '2015-05-04')
# invalid_dates <- c('2015-05-04_25-00-00', '2015-05-34',
#                   '2015-05-03 10:23:11')
parseDateStr <- function(str, tz=''){
  
  pattern <- "^[0-9]{4}-[0-9]{2}-[0-9]{2}(_[0-9]{2}-[0-9]{2}-[0-9]{2}){0,1}$"
  if(length(str) >1){
    stop("Dates must be checked one by one, you are providing several dates")
  }
  match = grep(pattern, str)
  if (length(match) != 1){
    stop(sprintf("Date '%s' is not formated correctly.
                 It must be either 'yyyy-mm-dd' or 'yyyy-mm-dd_hh-mm-ss'",str))
  }
  
  out <- list()
  
  if(nchar(str) == 10){
    date <- as.POSIXct(str, "%Y-%m-%d",tz=tz)
    out$date <- date
    out$has_time <- F
  }
  else{
    date <- as.POSIXct(str, "%Y-%m-%d_%H-%M-%S",tz=tz)
    out$date <- date
    out$has_time <- T
  }
  if(is.na(date)){
    stop(sprintf("Date '%s' seems to be formated correctly,
                 but we cannot read it as a date. 
                 Probably the numbers are wrong (e.g. 2015-30-02 does not exist)",str))
  }
  
  out
}


dateStrToPosix <- function(date,tz="GMT"){
  if(is.infinite(date))
    return(date)
  parseDateStr(as.character(date),tz)$date
}

# invalid_dates <- c('2015-05-04_25-00-00', '2015-05-34',
#                   '2015-05-03 10:23:11')
