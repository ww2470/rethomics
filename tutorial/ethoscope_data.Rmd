---
title: "Loading Ethoscope Data"
author: "Quentin Geissmann"
date: "2 March 2016"
output: html_document
---


Aims
----- 

In this tutorial, we will learn:

* How to load data from the ethoscope platform.
* How to use reference time to align experimental times
* How to apply functions on data as they are loaded
* Ways we can speed-up data import

Ethoscope platform
----------------------------------------
Like `rethomics`, the [ethoscope platform](http://gilestrolab.github.io/ethoscope/) is a tool
developed in the [Gilestro lab](http://lab.gilest.ro/), so they should integrate very well.

When working with the ethoscope platform, you will most likely end up with experiments with several devices at different times.
The platform saves one data file per experiment (*i.e.* one machine at one time) in an individual `.db` file (an sqlite3 db).
The files are all organised in a canonical hierarchy:

```
/machine-id/machine-name/datetime/datetime_machine-id.db
```

Listing experiments
----------------------------------------

```{r, include=FALSE}
library(rethomics)
source("rprint.R")
```

```{r,eval=FALSE}
# here, you need to write the path to the tutorial folder on your system
TUTO_DATA_DIR <- "/the/path/to/rethomics_tutorial_data"
```

```{r}
# we build the path to ethoscope results
# Note that you could just as well write the complete path yourself here.
result_dir <- paste(TUTO_DATA_DIR,"ethoscope_results", sep="/")
# we check the folder exists
if(!dir.exists(result_dir))
    stop(sprintf("No such directory: %s", result_dir))
```

By using `buildEthoscopeQuery` without any query, we can already list all the experiments:

```{r}
exp_list <- buildEthoscopeQuery(result_dir)
print(exp_list)
```

This tables shows three experiments(rows) and identify them by machine id, machine name, date and time, and the name of the file that stores the data.
You can see that it also contains the complete path to each file. This is likely to be different in your system.

Note that `exp_list` is a data table, so you can easly subset it:

```{r}
# experiments from machine 11 only
print(exp_list[machine_name == "ETHOSCOPE_011"])
# experiments after 10 Jan 2016
print(exp_list[date > "2016-01-10"])
```



Loading data from a query
----------------------------------------

As [we have already discussed](queries.html), you are expected to generate a query that describe the experimental conditions for each animal.

In order to load ethoscope data (via `loadEthoscopeData`), we need a query that contains:

* the **path** to the result file
* the **region id** so we can find any given animal within this experiment
* optionally, other variables like treatment, genotype,... that you can define yourself.

However, it is not great to write by hand a query with the path because:

* it is very easy to make mistakes
* it involves you going through the result files every time
* the path may be different for say a collaborator with whom you want to share data and queries

The alternative is to write a primary query that contains:

* the *machine name* (`machine_name`)
* the *date* (`date`)
* the *region id* (`region_id`)

Then you can build a secondary query using `buildEthoscopeQuery` that will automatically find your experiment in the result files.


Let's play with an example from the `sleep_query.csv` query.

```{r}
query_file <- paste(TUTO_DATA_DIR,"ethoscope_queries","sleep_query.csv",sep="/")
# one can use fread to load the query:
primary_query <- fread(query_file)
print(primary_query)
```

Here, we have 40 animals from three experiments (in two different days and two different machines)

The next step is generate the final query:

```{r}
# we need to tell R where to find the files (i.e. in result_dir)
final_query <- buildEthoscopeQuery(result_dir, primary_query)
# let us look at the columns in the final query
print(colnames(final_query))
```

You can see that the final query now has columns for `path`, `machine_id` and `file`.
Therefore, we are ready to load data.

The crucial function to load data is `loadEthoscopeData`.
We will not go into details, but you can type `?loadEthoscopeData` to see all the options.

The most basic way to load data is simply (this could take a while, so go and make a cup of tea):

```{r}
dt <- loadEthoscopeData(final_query, verbose=F)
# Here, I turned off verbose so that we don't fill the tutorial with progress reports
# Instead, you can simply use:
# dt <- loadEthoscopeData(final_query)
```

Now, all the data for this query is in `dt`.

```{r}
# you can print information about the data:
colnames(dt)
nrow(dt)
key(dt)
```

Also let's print the first 100 rows to have a lok at the structure:
```{r}
print(dt[1:100])
```


This tells us which are the variables (columns), the number of observations (number of rows) and what is the key, which identifies individual animals.

You will notice that there are several behavioural variables that are generated by the tracking algorithm such as `x`,`y`,`w`,`h` and others.


The reference time issue
--------------------------------

**Ethoscope data uses GMT** instead of local time. This means that the time will not change according to timezones or season.
This may seem a bit conterintuintive at first, but it avoids a lot of bugs (*e.g.* when the time changes between winter and summer time during an experiment, or when sharing data with collaborators from another time zone).

By default, the time variable, `t` is expressed in second since the start of the experiments.
This may make sense in many cases (for instance, in short experiments where we want to see what happens after one hour in the arena).
However, for many long experiments, we want to align time to a circadian reference.
In our case, experiments done on the 2016-01-08th started around 16:05:00 GMT, whilst the one on the 2016-01-15th started around 18:15:00 GMT.

The option `reference_hour` in `loadEthoscopeData` allows you to force t0 to be at a given time of the day.
By convention, we pick ZT0 for the reference hour. That means that the time will now be expressed relatively to the start of the first biological day of each experiment.
For example, in our lab, light turns on at 09:00:00 GMT, so we can use `9.00 as a reference hour`.


```{r, eval=F}
dt <- loadEthoscopeData(final_query, reference_hour=9.0, verbose=F)
print(dt[1:100])
```


Preprocessing data on the fly
--------------------------------------
As you can see, `dt` has a huge number of observations, which take a lot of space in memory.
In many instances, you already know what type of analysis you are going to perform, and you need
to summarise/ downsample time series.

For example, if you perform movement and sleep scoring in fruit fly, you will score every 10s of behaviour.
Therefore, you should have one observation every 10s instead of around every 0.5s.


```{r}
dt <- loadEthoscopeData(final_query, reference_hour=9.0, FUN=sleepAnnotation, verbose=F, masking_duration=0)
print(colnames(dt))
print(dt[1:100])
```

This means `FUN` is a function (and you can define your own!) that will be applied on the data from each animal immediately after the data is loaded.


Speeding up data import
---------------------------

There are different way to read data faster.

The first one is to select only columns that you will (or your preprocessing function) need from the data.
Here, for instance, we keep the `x` position and the variable used by `sleepAnnotation` to quantify motion(`xy_dist_log10x1000`).

```{r}
dt <- loadEthoscopeData(final_query,
                        reference_hour=9.0,
                        columns=c("xy_dist_log10x1000","x"),
                        FUN=sleepAnnotation,
                        verbose=F,
                        masking_duration=0)

print(dt[1:100])
```

Another way to speed up data reading is to use several cores in parallel. This can be achieved using `ncores=N`, where N is the number of parallel processes.
Note that this will result in a *larger memory footprint*, but is it convenient when you have loads of RAM and little time.


For completeness, I should also mention that there is a way to preload databases files and save them as native `R` objects, but this is a bit technical, so I will not go into detail.
If you are interested, you can find information in `?cacheEthoscopeData`.
